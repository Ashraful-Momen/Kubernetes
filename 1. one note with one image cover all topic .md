# Kubernetes Production Master Guide
*From Docker to Kubernetes Mastery in 1 Week - Using Your Nginx App*

> **Learning Path**: We'll use your existing nginx Docker image throughout this guide to demonstrate every Kubernetes concept - from basic deployment to advanced production patterns.

## Prerequisites
- Your nginx image: `your-dockerhub-username/nginx-app:latest`
- Basic Docker knowledge ✅
- AWS account (for EKS section)

---

## Table of Contents
1. [Cluster Architecture & Setup](#cluster-architecture--setup)
2. [Essential kubectl Commands](#essential-kubectl-commands)
3. [Pods - Your First Deployment](#pods---your-first-deployment)
4. [Deployments - Production Workloads](#deployments---production-workloads)
5. [Services - Internal Networking](#services---internal-networking)
6. [Ingress - External Access](#ingress---external-access)
7. [Gateway API - Next-Gen Routing](#gateway-api---next-gen-routing)
8. [ConfigMaps & Secrets - Configuration Management](#configmaps--secrets---configuration-management)
9. [Volumes & Storage - Data Persistence](#volumes--storage---data-persistence)
10. [Jobs & CronJobs - Batch Processing](#jobs--cronjobs---batch-processing)
11. [DaemonSets - Node-Level Services](#daemonsets---node-level-services)
12. [Auto-scaling - Dynamic Resource Management](#auto-scaling---dynamic-resource-management)
13. [Monitoring & Logging - Observability](#monitoring--logging---observability)
14. [Security & RBAC - Access Control](#security--rbac---access-control)
15. [Advanced Patterns - Production Excellence](#advanced-patterns---production-excellence)
16. [AWS EKS - Managed Kubernetes](#aws-eks---managed-kubernetes)
17. [Helm & Templating - Package Management](#helm--templating---package-management)
18. [Troubleshooting - Production Issues](#troubleshooting---production-issues)

---

## Cluster Architecture & Setup

### **What**: Kubernetes cluster components and how master nodes control worker nodes
### **Why**: Understanding the foundation before building applications
### **Network**: Internal cluster communication between control plane and data plane

```bash
# Cluster overview
kubectl cluster-info
kubectl get nodes -o wide
kubectl get componentstatuses
```

### Production kubeadm Setup

#### Master Node Setup
```bash
# Initialize master node
sudo kubeadm init \
  --apiserver-advertise-address=10.0.1.10 \
  --pod-network-cidr=192.168.0.0/16 \
  --service-cidr=10.96.0.0/12 \
  --kubernetes-version=v1.28.0

# Setup kubectl for root
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# Install CNI (Calico for production)
kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.0/manifests/calico.yaml
```

#### Worker Node Binding
```bash
# On master - get join command
kubeadm token create --print-join-command

# On worker nodes - join cluster
sudo kubeadm join 10.0.1.10:6443 \
  --token abcdef.1234567890abcdef \
  --discovery-token-ca-cert-hash sha256:hash_value

# Verify nodes joined
kubectl get nodes
kubectl describe node worker-1
```

#### High Availability Setup
```yaml
# /etc/kubernetes/kubeadm-config.yaml
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
kubernetesVersion: v1.28.0
controlPlaneEndpoint: loadbalancer.example.com:6443
networking:
  podSubnet: 192.168.0.0/16
  serviceSubnet: 10.96.0.0/12
etcd:
  external:
    endpoints:
    - https://etcd1.example.com:2379
    - https://etcd2.example.com:2379
    - https://etcd3.example.com:2379
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 10.0.1.10
```

---

## Essential kubectl Commands

### **What**: Command-line interface to interact with Kubernetes API
### **Why**: Primary tool for managing Kubernetes resources
### **Use Cases**: Daily operations, debugging, automation

```bash
# Context management
kubectl config get-contexts
kubectl config use-context production-cluster
kubectl config set-context --current --namespace=nginx-app

# Resource operations
kubectl get all -o wide
kubectl describe <resource> <name>
kubectl logs -f deployment/nginx-app
kubectl exec -it <pod-name> -- /bin/bash

# Apply configurations
kubectl apply -f nginx-deployment.yaml
kubectl diff -f nginx-deployment.yaml
kubectl delete -f nginx-deployment.yaml

# Useful aliases for daily work
alias k='kubectl'
alias kg='kubectl get'
alias kd='kubectl describe'
alias kl='kubectl logs'
alias ka='kubectl apply -f'
```

---

## Pods - Your First Deployment

### **What**: Smallest deployable unit containing one or more containers
### **Why**: Foundation of all Kubernetes workloads
### **Network**: Each pod gets unique IP within cluster network
### **Use Cases**: Running your nginx container, sidecar patterns, init containers

### Basic Pod for Your Nginx App
```yaml
# nginx-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-app-pod
  labels:
    app: nginx-app
    version: v1
    environment: learning
spec:
  containers:
  - name: nginx
    image: your-dockerhub-username/nginx-app:latest
    ports:
    - containerPort: 80
      name: http
    env:
    - name: ENVIRONMENT
      value: "development"
    resources:
      requests:
        memory: "64Mi"
        cpu: "50m"
      limits:
        memory: "128Mi"
        cpu: "100m"
    # Health checks
    livenessProbe:
      httpGet:
        path: /
        port: 80
      initialDelaySeconds: 10
      periodSeconds: 10
    readinessProbe:
      httpGet:
        path: /
        port: 80
      initialDelaySeconds: 5
      periodSeconds: 5
  restartPolicy: Always
```

### Advanced Pod with Init Container
```yaml
# nginx-pod-advanced.yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-app-advanced
  labels:
    app: nginx-app
spec:
  # Init container to prepare content
  initContainers:
  - name: content-setup
    image: busybox:1.35
    command: ['sh', '-c']
    args:
    - |
      echo "Setting up nginx content..."
      echo "<h1>Hello from Kubernetes!</h1>" > /shared/index.html
      echo "Pod: $HOSTNAME" >> /shared/index.html
      echo "Time: $(date)" >> /shared/index.html
    volumeMounts:
    - name: shared-content
      mountPath: /shared
  containers:
  - name: nginx
    image: nginx:1.24-alpine
    ports:
    - containerPort: 80
    volumeMounts:
    - name: shared-content
      mountPath: /usr/share/nginx/html
    resources:
      requests:
        memory: "64Mi"
        cpu: "50m"
      limits:
        memory: "128Mi"
        cpu: "100m"
  volumes:
  - name: shared-content
    emptyDir: {}
```

### Commands
```bash
# Deploy and test pod
kubectl apply -f nginx-pod.yaml
kubectl get pods -o wide
kubectl describe pod nginx-app-pod
kubectl logs nginx-app-pod
kubectl port-forward nginx-app-pod 8080:80

# Test in browser: http://localhost:8080

# Debug pod
kubectl exec -it nginx-app-pod -- /bin/bash
kubectl exec nginx-app-pod -- ls -la /usr/share/nginx/html
```

---

## Deployments - Production Workloads

### **What**: Manages replica sets and rolling updates for stateless applications
### **Why**: Production-ready scaling, updates, and self-healing
### **Network**: Load balances traffic across multiple pod replicas
### **Use Cases**: Web applications, APIs, microservices

### Production Nginx Deployment
```yaml
# nginx-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-app
  namespace: production
  labels:
    app: nginx-app
    version: v1
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  selector:
    matchLabels:
      app: nginx-app
  template:
    metadata:
      labels:
        app: nginx-app
        version: v1
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9113"
        prometheus.io/path: "/metrics"
    spec:
      # Ensure pods spread across nodes
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - nginx-app
              topologyKey: kubernetes.io/hostname
      containers:
      - name: nginx
        image: your-dockerhub-username/nginx-app:latest
        imagePullPolicy: Always
        ports:
        - containerPort: 80
          name: http
        env:
        - name: ENVIRONMENT
          value: "production"
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"
        # Production health checks
        livenessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
        # Graceful shutdown
        lifecycle:
          preStop:
            exec:
              command: ["/bin/sh", "-c", "sleep 10"]
        volumeMounts:
        - name: nginx-config
          mountPath: /etc/nginx/conf.d
        - name: nginx-logs
          mountPath: /var/log/nginx
      # Nginx metrics exporter sidecar
      - name: nginx-exporter
        image: nginx/nginx-prometheus-exporter:0.11.0
        args:
        - -nginx.scrape-uri=http://localhost:80/nginx_status
        ports:
        - containerPort: 9113
          name: metrics
        resources:
          requests:
            memory: "32Mi"
            cpu: "25m"
          limits:
            memory: "64Mi"
            cpu: "50m"
      volumes:
      - name: nginx-config
        configMap:
          name: nginx-config
      - name: nginx-logs
        emptyDir: {}
      # Security context
      securityContext:
        runAsNonRoot: true
        runAsUser: 101
        fsGroup: 101
      terminationGracePeriodSeconds: 30
```

### Deployment Management Commands
```bash
# Create namespace and deploy
kubectl create namespace production
kubectl apply -f nginx-deployment.yaml

# Monitor deployment
kubectl rollout status deployment/nginx-app -n production
kubectl get deployment nginx-app -n production -o wide
kubectl get pods -n production -l app=nginx-app

# Scaling
kubectl scale deployment nginx-app --replicas=5 -n production

# Rolling updates
kubectl set image deployment/nginx-app nginx=your-dockerhub-username/nginx-app:v2 -n production
kubectl rollout history deployment/nginx-app -n production
kubectl rollout undo deployment/nginx-app -n production

# Debug deployment issues
kubectl describe deployment nginx-app -n production
kubectl get events -n production --sort-by=.metadata.creationTimestamp
```

---

## Services - Internal Networking

### **What**: Stable network endpoint for accessing pods
### **Why**: Pods are ephemeral; services provide consistent access
### **Network Types**: ClusterIP (internal), NodePort (node access), LoadBalancer (external)
### **Use Cases**: Service discovery, load balancing, internal communication

### ClusterIP Service (Internal Access)
```yaml
# nginx-service-clusterip.yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx-app-service
  namespace: production
  labels:
    app: nginx-app
spec:
  type: ClusterIP
  selector:
    app: nginx-app
  ports:
  - name: http
    port: 80
    targetPort: 80
    protocol: TCP
  - name: metrics
    port: 9113
    targetPort: 9113
    protocol: TCP
  sessionAffinity: None
```

### NodePort Service (Node Access)
```yaml
# nginx-service-nodeport.yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx-app-nodeport
  namespace: production
spec:
  type: NodePort
  selector:
    app: nginx-app
  ports:
  - name: http
    port: 80
    targetPort: 80
    nodePort: 30080
    protocol: TCP
```

### LoadBalancer Service (Cloud Provider)
```yaml
# nginx-service-lb.yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx-app-lb
  namespace: production
  annotations:
    # AWS Load Balancer Controller annotations
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: "http"
    service.beta.kubernetes.io/aws-load-balancer-healthcheck-path: "/"
    service.beta.kubernetes.io/aws-load-balancer-scheme: "internet-facing"
spec:
  type: LoadBalancer
  selector:
    app: nginx-app
  ports:
  - port: 80
    targetPort: 80
    protocol: TCP
  loadBalancerSourceRanges:
  - 0.0.0.0/0  # Allow all (restrict in production)
```

### Headless Service (DNS Discovery)
```yaml
# nginx-service-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx-app-headless
  namespace: production
spec:
  clusterIP: None
  selector:
    app: nginx-app
  ports:
  - port: 80
    targetPort: 80
```

### Service Commands
```bash
# Deploy services
kubectl apply -f nginx-service-clusterip.yaml
kubectl apply -f nginx-service-nodeport.yaml

# Check services
kubectl get services -n production
kubectl describe service nginx-app-service -n production
kubectl get endpoints -n production

# Test internal connectivity
kubectl run test-pod --image=busybox -n production -it --rm -- sh
# Inside pod: wget -qO- nginx-app-service.production.svc.cluster.local

# Port forwarding for testing
kubectl port-forward service/nginx-app-service 8080:80 -n production
# Test: http://localhost:8080

# Test NodePort (if using NodePort service)
# Access via: http://<node-ip>:30080
```

---

## Ingress - External Access

### **What**: HTTP/HTTPS routing to services from outside the cluster
### **Why**: Single entry point, SSL termination, path-based routing
### **Network**: Layer 7 (application layer) load balancing
### **Use Cases**: Web applications, APIs, multi-tenant applications

### Nginx Ingress Controller Setup
```yaml
# nginx-ingress-controller.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: ingress-nginx
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-ingress-controller
  namespace: ingress-nginx
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx-ingress-controller
  template:
    metadata:
      labels:
        app: nginx-ingress-controller
    spec:
      containers:
      - name: nginx-ingress-controller
        image: k8s.gcr.io/ingress-nginx/controller:v1.8.1
        args:
        - /nginx-ingress-controller
        - --ingress-class=nginx
        - --configmap=$(POD_NAMESPACE)/nginx-configuration
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        ports:
        - name: http
          containerPort: 80
        - name: https
          containerPort: 443
```

### Basic Ingress for Your Nginx App
```yaml
# nginx-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: nginx-app-ingress
  namespace: production
  annotations:
    kubernetes.io/ingress.class: "nginx"
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
spec:
  rules:
  - host: nginx-app.local
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: nginx-app-service
            port:
              number: 80
```

### Advanced Ingress with SSL and Multiple Paths
```yaml
# nginx-ingress-advanced.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: nginx-app-ingress-advanced
  namespace: production
  annotations:
    kubernetes.io/ingress.class: "nginx"
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    nginx.ingress.kubernetes.io/rate-limit: "100"
    nginx.ingress.kubernetes.io/rate-limit-window: "1m"
    nginx.ingress.kubernetes.io/configuration-snippet: |
      more_set_headers "X-Frame-Options: DENY";
      more_set_headers "X-Content-Type-Options: nosniff";
      more_set_headers "X-XSS-Protection: 1; mode=block";
spec:
  tls:
  - hosts:
    - app.example.com
    secretName: nginx-app-tls
  rules:
  - host: app.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: nginx-app-service
            port:
              number: 80
      - path: /metrics
        pathType: Prefix
        backend:
          service:
            name: nginx-app-service
            port:
              number: 9113
      - path: /health
        pathType: Exact
        backend:
          service:
            name: nginx-app-service
            port:
              number: 80
```

### Ingress Commands
```bash
# Deploy ingress
kubectl apply -f nginx-ingress.yaml

# Check ingress
kubectl get ingress -n production
kubectl describe ingress nginx-app-ingress -n production

# Test ingress (add to /etc/hosts: <ingress-ip> nginx-app.local)
curl -H "Host: nginx-app.local" http://<ingress-controller-ip>/

# Check ingress controller logs
kubectl logs -n ingress-nginx deployment/nginx-ingress-controller

# Get ingress controller IP
kubectl get service -n ingress-nginx
```

---

## Gateway API - Next-Gen Routing

### **What**: Next-generation ingress with more expressive routing
### **Why**: Better separation of concerns, more advanced traffic management
### **Network**: Layer 4 and Layer 7 routing with traffic splitting
### **Use Cases**: Advanced routing, A/B testing, canary deployments

### Gateway Setup for Nginx App
```yaml
# nginx-gateway.yaml
apiVersion: gateway.networking.k8s.io/v1beta1
kind: Gateway
metadata:
  name: nginx-app-gateway
  namespace: production
spec:
  gatewayClassName: istio
  listeners:
  - name: http
    port: 80
    protocol: HTTP
    allowedRoutes:
      namespaces:
        from: Same
  - name: https
    port: 443
    protocol: HTTPS
    tls:
      mode: Terminate
      certificateRefs:
      - name: nginx-app-tls
    allowedRoutes:
      namespaces:
        from: Same
---
apiVersion: gateway.networking.k8s.io/v1beta1
kind: HTTPRoute
metadata:
  name: nginx-app-route
  namespace: production
spec:
  parentRefs:
  - name: nginx-app-gateway
  hostnames:
  - app.example.com
  rules:
  - matches:
    - path:
        type: PathPrefix
        value: /
    backendRefs:
    - name: nginx-app-service
      port: 80
      weight: 90
    - name: nginx-app-service-canary
      port: 80
      weight: 10  # 10% canary traffic
  - matches:
    - path:
        type: Exact
        value: /metrics
    backendRefs:
    - name: nginx-app-service
      port: 9113
```

### Gateway Commands
```bash
# Install Gateway API CRDs
kubectl apply -f https://github.com/kubernetes-sigs/gateway-api/releases/download/v0.8.0/standard-install.yaml

# Deploy gateway
kubectl apply -f nginx-gateway.yaml

# Check gateway status
kubectl get gateway -n production
kubectl describe gateway nginx-app-gateway -n production
kubectl get httproute -n production
```

---

## ConfigMaps & Secrets - Configuration Management

### **What**: External configuration and sensitive data management
### **Why**: Separate config from code, secure sensitive data
### **Use Cases**: App configuration, database credentials, certificates

### Nginx ConfigMap
```yaml
# nginx-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-config
  namespace: production
data:
  nginx.conf: |
    events {
        worker_connections 1024;
    }
    http {
        include       /etc/nginx/mime.types;
        default_type  application/octet-stream;
        
        log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                        '$status $body_bytes_sent "$http_referer" '
                        '"$http_user_agent" "$http_x_forwarded_for"';
        
        access_log /var/log/nginx/access.log main;
        error_log /var/log/nginx/error.log warn;
        
        sendfile on;
        tcp_nopush on;
        tcp_nodelay on;
        keepalive_timeout 65;
        types_hash_max_size 2048;
        
        gzip on;
        gzip_types text/plain text/css application/json application/javascript text/xml application/xml;
        
        server {
            listen 80;
            server_name _;
            
            location / {
                root /usr/share/nginx/html;
                index index.html index.htm;
                try_files $uri $uri/ =404;
            }
            
            location /nginx_status {
                stub_status on;
                access_log off;
                allow 127.0.0.1;
                allow 10.0.0.0/8;
                deny all;
            }
            
            location /health {
                access_log off;
                return 200 "healthy\n";
                add_header Content-Type text/plain;
            }
        }
    }
  index.html: |
    <!DOCTYPE html>
    <html>
    <head>
        <title>Nginx on Kubernetes</title>
        <style>
            body { font-family: Arial, sans-serif; margin: 40px; }
            .container { max-width: 800px; margin: 0 auto; }
            .header { background: #f4f4f4; padding: 20px; border-radius: 5px; }
            .info { margin: 20px 0; padding: 15px; background: #e7f3ff; border-radius: 5px; }
        </style>
    </head>
    <body>
        <div class="container">
            <div class="header">
                <h1>🚀 Nginx on Kubernetes</h1>
                <p>Your Docker image successfully deployed!</p>
            </div>
            <div class="info">
                <h3>Pod Information:</h3>
                <p><strong>Environment:</strong> {{ENVIRONMENT}}</p>
                <p><strong>Pod Name:</strong> {{POD_NAME}}</p>
                <p><strong>Pod IP:</strong> {{POD_IP}}</p>
                <p><strong>Timestamp:</strong> {{TIMESTAMP}}</p>
            </div>
        </div>
    </body>
    </html>
```

### Secrets for SSL and Database
```yaml
# nginx-secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: nginx-app-secrets
  namespace: production
type: Opaque
data:
  database-password: cGFzc3dvcmQxMjM=  # base64: password123
  api-key: YWJjZGVmZ2hpams=  # base64: abcdefghijk
---
apiVersion: v1
kind: Secret
metadata:
  name: nginx-app-tls
  namespace: production
type: kubernetes.io/tls
data:
  tls.crt: LS0tLS1CRUdJTi...  # base64 encoded certificate
  tls.key: LS0tLS1CRUdJTi...  # base64 encoded private key
---
apiVersion: v1
kind: Secret
metadata:
  name: docker-registry-secret
  namespace: production
type: kubernetes.io/dockerconfigjson
data:
  .dockerconfigjson: ewogICJhdXRocyI6IHsKICAgICJyZWdpc3RyeS5leGFtcGxlLmNvbSI6IHsKICAgICAgImF1dGgiOiAiWVdSdGFXNDZjR0Z6YzNkdmNtUT0iCiAgICB9CiAgfQp9
```

### Updated Deployment with ConfigMap and Secrets
```yaml
# nginx-deployment-with-config.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-app
  namespace: production
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-app
  template:
    metadata:
      labels:
        app: nginx-app
    spec:
      imagePullSecrets:
      - name: docker-registry-secret
      containers:
      - name: nginx
        image: your-dockerhub-username/nginx-app:latest
        ports:
        - containerPort: 80
        env:
        - name: ENVIRONMENT
          value: "production"
        - name: DATABASE_PASSWORD
          valueFrom:
            secretKeyRef:
              name: nginx-app-secrets
              key: database-password
        - name: API_KEY
          valueFrom:
            secretKeyRef:
              name: nginx-app-secrets
              key: api-key
        volumeMounts:
        - name: nginx-config
          mountPath: /etc/nginx/nginx.conf
          subPath: nginx.conf
        - name: nginx-content
          mountPath: /usr/share/nginx/html/index.html
          subPath: index.html
      volumes:
      - name: nginx-config
        configMap:
          name: nginx-config
      - name: nginx-content
        configMap:
          name: nginx-config
```

### ConfigMap/Secret Commands
```bash
# Create from command line
kubectl create configmap nginx-config --from-file=nginx.conf -n production
kubectl create secret generic nginx-secrets --from-literal=db-password=secret123 -n production

# Create TLS secret from files
kubectl create secret tls nginx-app-tls --cert=tls.crt --key=tls.key -n production

# Deploy all configs
kubectl apply -f nginx-configmap.yaml
kubectl apply -f nginx-secrets.yaml

# View configurations
kubectl get configmap nginx-config -n production -o yaml
kubectl describe secret nginx-app-secrets -n production

# Decode secret
kubectl get secret nginx-app-secrets -n production -o jsonpath='{.data.database-password}' | base64 -d

# Edit configurations
kubectl edit configmap nginx-config -n production
```

---

## Volumes & Storage - Data Persistence

### **What**: Persistent data storage for containers
### **Why**: Data survives pod restarts and rescheduling
### **Types**: emptyDir (temp), hostPath (node), PV/PVC (persistent), cloud storage
### **Use Cases**: Databases, file uploads, logs, static content

### Storage Class for Fast SSD
```yaml
# storage-class.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast-ssd
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp3
  iops: "3000"
  throughput: "125"
  encrypted: "true"
allowVolumeExpansion: true
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer
```

### Persistent Volume for Nginx Logs
```yaml
# nginx-pv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nginx-logs-pv
  labels:
    app: nginx-app
spec:
  capacity:
    storage: 5Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: fast-ssd
  hostPath:
    path: /data/nginx-logs
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nginx-logs-pvc
  namespace: production
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  storageClassName: fast-ssd
  selector:
    matchLabels:
      app: nginx-app
```

### Nginx with Persistent Storage
```yaml
# nginx-deployment-with-storage.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-app-with-storage
  namespace: production
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-app
  template:
    metadata:
      labels:
        app: nginx-app
    spec:
      containers:
      - name: nginx
        image: your-dockerhub-username/nginx-app:latest
        ports:
        - containerPort: 80
        volumeMounts:
        # Persistent storage for logs
        - name: nginx-logs
          mountPath: /var/log/nginx
        # ConfigMap for configuration
        - name: nginx-config
          mountPath: /etc/nginx/nginx.conf
          subPath: nginx.conf
        # EmptyDir for temporary files
        - name: tmp-storage
          mountPath: /tmp
        # Secret for SSL certificates
        - name: ssl-certs
          mountPath: /etc/ssl/certs
          readOnly: true
      # Sidecar for log processing
      - name: log-processor
        image: busybox:1.35
        command: ["/bin/sh"]
        args:
        - -c
        - |
          while true; do
            echo "Processing logs at $(date)"
            find /var/log/nginx -name "*.log" -type f -exec tail -n 10 {} \;
            sleep 300
          done
        volumeMounts:
        - name: nginx-logs
          mountPath: /var/log/nginx
          readOnly: true
      volumes:
      - name: nginx-logs
        persistentVolumeClaim:
          claimName: nginx-logs-pvc
      - name: nginx-config
        configMap:
          name: nginx-config
      - name: tmp-storage
        emptyDir:
          sizeLimit: 1Gi
      - name: ssl-certs
        secret:
          secretName: nginx-app-tls
          defaultMode: 0400
```

### StatefulSet for Persistent Nginx
```yaml
# nginx-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: nginx-stateful
  namespace: production
spec:
  serviceName: nginx-headless
  replicas: 3
  selector:
    matchLabels:
      app: nginx-stateful
  template:
    metadata:
      labels:
        app: nginx-stateful
    spec:
      containers:
      - name: nginx
        image: your-dockerhub-username/nginx-app:latest
        ports:
        - containerPort: 80
        volumeMounts:
        - name: nginx-storage
          mountPath: /usr/share/nginx/html
        - name: nginx-logs
          mountPath: /var/log/nginx
  volumeClaimTemplates:
  - metadata:
      name: nginx-storage
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 1Gi
  - metadata:
      name: nginx-logs
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 2Gi
```

### Storage Commands
```bash
# Deploy storage resources
kubectl apply -f storage-class.yaml
kubectl apply -f nginx-pv.yaml

# Check storage
kubectl get pv
kubectl get pvc -n production
kubectl get storageclass

# Monitor storage usage
kubectl describe pvc nginx-logs-pvc -n production
kubectl get pvc -n production -o custom-columns=NAME:.metadata.name,CAPACITY:.spec.resources.requests.storage,STATUS:.status.phase

# Access pod storage
kubectl exec -it nginx-app-with-storage-xxx -n production -- ls -la /var/log/nginx
kubectl exec -it nginx-app-with-storage-xxx -n production -c log-processor -- tail -f /var/log/nginx/access.log
```

---

## Jobs & CronJobs - Batch Processing

### **What**: Run batch workloads and scheduled tasks
### **Why**: Data processing, backups, maintenance tasks
### **Use Cases**: Database backups, data migration, periodic cleanup

### One-time Job - Nginx Config Validation
```yaml
# nginx-config-validation-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: nginx-config-validation
  namespace: production
spec:
  completions: 1
  parallelism: 1
  backoffLimit: 3
  template:
    metadata:
      labels:
        app: nginx-config-validation
    spec:
      restartPolicy: Never
      containers:
      - name: validator
        image: nginx:1.24-alpine
        command: ["/bin/sh"]
        args:
        - -c
        - |
          echo "Validating nginx configuration..."
          nginx -t -c /etc/nginx/nginx.conf
          if [ $? -eq 0 ]; then
            echo "✅ Configuration is valid"
          else
            echo "❌ Configuration has errors"
            exit 1
          fi
        volumeMounts:
        - name: nginx-config
          mountPath: /etc/nginx/nginx.conf
          subPath: nginx.conf
      volumes:
      - name: nginx-config
        configMap:
          name: nginx-config
```

### CronJob - Log Rotation
```yaml
# nginx-log-rotation-cronjob.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: nginx-log-rotation
  namespace: production
spec:
  schedule: "0 2 * * *"  # Daily at 2 AM
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: nginx-log-rotation
        spec:
          restartPolicy: OnFailure
          containers:
          - name: log-rotator
            image: busybox:1.35
            command: ["/bin/sh"]
            args:
            - -c
            - |
              echo "Starting log rotation at $(date)"
              cd /var/log/nginx
              
              # Rotate access logs
              if [ -f access.log ]; then
                mv access.log access.log.$(date +%Y%m%d-%H%M%S)
                echo "Rotated access.log"
              fi
              
              # Rotate error logs
              if [ -f error.log ]; then
                mv error.log error.log.$(date +%Y%m%d-%H%M%S)
                echo "Rotated error.log"
              fi
              
              # Keep only last 7 days of logs
              find . -name "*.log.*" -mtime +7 -delete
              echo "Cleaned up old logs"
              
              # Send HUP signal to nginx to reopen log files
              killall -HUP nginx 2>/dev/null || true
              echo "Log rotation completed at $(date)"
            volumeMounts:
            - name: nginx-logs
              mountPath: /var/log/nginx
          volumes:
          - name: nginx-logs
            persistentVolumeClaim:
              claimName: nginx-logs-pvc
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 1
```

### Parallel Job - Load Testing
```yaml
# nginx-load-test-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: nginx-load-test
  namespace: production
spec:
  completions: 10
  parallelism: 5
  template:
    metadata:
      labels:
        app: nginx-load-test
    spec:
      restartPolicy: Never
      containers:
      - name: load-tester
        image: busybox:1.35
        command: ["/bin/sh"]
        args:
        - -c
        - |
          echo "Starting load test from pod $HOSTNAME"
          for i in $(seq 1 100); do
            wget -q -O- http://nginx-app-service.production.svc.cluster.local/ > /dev/null
            if [ $? -eq 0 ]; then
              echo "Request $i: SUCCESS"
            else
              echo "Request $i: FAILED"
            fi
            sleep 0.1
          done
          echo "Load test completed from pod $HOSTNAME"
```

### Job Commands
```bash
# Run one-time job
kubectl apply -f nginx-config-validation-job.yaml
kubectl get jobs -n production
kubectl logs job/nginx-config-validation -n production

# Manage CronJobs
kubectl apply -f nginx-log-rotation-cronjob.yaml
kubectl get cronjobs -n production
kubectl describe cronjob nginx-log-rotation -n production

# Check job history
kubectl get jobs -n production --show-labels
kubectl logs job/nginx-log-rotation-xxx -n production

# Manually trigger CronJob
kubectl create job --from=cronjob/nginx-log-rotation manual-log-rotation -n production

# Clean up completed jobs
kubectl delete job nginx-config-validation -n production
```

---

## DaemonSets - Node-Level Services

### **What**: Runs one pod per node (or subset of nodes)
### **Why**: Node-level services like monitoring, logging, security
### **Use Cases**: Log collection, monitoring agents, network plugins

### Nginx Log Collector DaemonSet
```yaml
# nginx-log-collector-daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nginx-log-collector
  namespace: production
  labels:
    app: nginx-log-collector
spec:
  selector:
    matchLabels:
      app: nginx-log-collector
  template:
    metadata:
      labels:
        app: nginx-log-collector
    spec:
      tolerations:
      # Allow running on master nodes
      - key: node-role.kubernetes.io/control-plane
        effect: NoSchedule
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      containers:
      - name: log-collector
        image: fluent/fluent-bit:2.1.8
        env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        resources:
          requests:
            memory: "64Mi"
            cpu: "50m"
          limits:
            memory: "128Mi"
            cpu: "100m"
        volumeMounts:
        - name: varlog
          mountPath: /var/log
          readOnly: true
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: fluent-bit-config
          mountPath: /fluent-bit/etc
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: fluent-bit-config
        configMap:
          name: fluent-bit-config
      serviceAccountName: fluent-bit
      terminationGracePeriodSeconds: 10
---
# ServiceAccount for log collector
apiVersion: v1
kind: ServiceAccount
metadata:
  name: fluent-bit
  namespace: production
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: fluent-bit
rules:
- apiGroups: [""]
  resources: ["pods", "nodes", "namespaces"]
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: fluent-bit
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: fluent-bit
subjects:
- kind: ServiceAccount
  name: fluent-bit
  namespace: production
```

### Node Exporter DaemonSet for Monitoring
```yaml
# node-exporter-daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-exporter
  namespace: monitoring
spec:
  selector:
    matchLabels:
      app: node-exporter
  template:
    metadata:
      labels:
        app: node-exporter
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9100"
    spec:
      tolerations:
      - effect: NoSchedule
        operator: Exists
      containers:
      - name: node-exporter
        image: prom/node-exporter:v1.6.1
        args:
        - --path.procfs=/host/proc
        - --path.sysfs=/host/sys
        - --path.rootfs=/host/root
        - --collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)
        ports:
        - containerPort: 9100
          name: metrics
        resources:
          requests:
            memory: "32Mi"
            cpu: "25m"
          limits:
            memory: "64Mi"
            cpu: "50m"
        volumeMounts:
        - name: proc
          mountPath: /host/proc
          readOnly: true
        - name: sys
          mountPath: /host/sys
          readOnly: true
        - name: root
          mountPath: /host/root
          mountPropagation: HostToContainer
          readOnly: true
      volumes:
      - name: proc
        hostPath:
          path: /proc
      - name: sys
        hostPath:
          path: /sys
      - name: root
        hostPath:
          path: /
      hostNetwork: true
      hostPID: true
```

### DaemonSet Commands
```bash
# Deploy DaemonSet
kubectl apply -f nginx-log-collector-daemonset.yaml

# Check DaemonSet status
kubectl get daemonset -n production
kubectl describe daemonset nginx-log-collector -n production

# Check pods on each node
kubectl get pods -n production -l app=nginx-log-collector -o wide

# View logs from all nodes
kubectl logs -n production -l app=nginx-log-collector

# Update DaemonSet
kubectl set image daemonset/nginx-log-collector log-collector=fluent/fluent-bit:2.1.9 -n production
kubectl rollout status daemonset/nginx-log-collector -n production
```

---

## Auto-scaling - Dynamic Resource Management

### **What**: Automatically adjust resources based on metrics
### **Why**: Handle varying load, optimize costs
### **Types**: HPA (horizontal), VPA (vertical), Cluster Autoscaler (nodes)

### Horizontal Pod Autoscaler for Nginx
```yaml
# nginx-hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: nginx-app-hpa
  namespace: production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx-app
  minReplicas: 2
  maxReplicas: 20
  metrics:
  # CPU utilization
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  # Memory utilization
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  # Custom metric - requests per second
  - type: Pods
    pods:
      metric:
        name: nginx_http_requests_per_second
      target:
        type: AverageValue
        averageValue: "100"
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
      - type: Pods
        value: 4
        periodSeconds: 15
      selectPolicy: Max
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
      - type: Pods
        value: 2
        periodSeconds: 60
      selectPolicy: Min
```

### Vertical Pod Autoscaler for Nginx
```yaml
# nginx-vpa.yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: nginx-app-vpa
  namespace: production
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx-app
  updatePolicy:
    updateMode: "Auto"  # Auto, Recreation, Initial, Off
  resourcePolicy:
    containerPolicies:
    - containerName: nginx
      minAllowed:
        cpu: 50m
        memory: 64Mi
      maxAllowed:
        cpu: 1
        memory: 512Mi
      controlledResources: ["cpu", "memory"]
      controlledValues: RequestsAndLimits
    - containerName: nginx-exporter
      minAllowed:
        cpu: 25m
        memory: 32Mi
      maxAllowed:
        cpu: 100m
        memory: 128Mi
      controlledResources: ["cpu", "memory"]
      controlledValues: RequestsAndLimits
```

### Pod Disruption Budget
```yaml
# nginx-pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: nginx-app-pdb
  namespace: production
spec:
  minAvailable: 2
  # OR use maxUnavailable: 1
  selector:
    matchLabels:
      app: nginx-app
```

### Load Testing for HPA
```yaml
# load-test-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: load-tester
  namespace: production
spec:
  replicas: 1
  selector:
    matchLabels:
      app: load-tester
  template:
    metadata:
      labels:
        app: load-tester
    spec:
      containers:
      - name: load-tester
        image: busybox:1.35
        command: ["/bin/sh"]
        args:
        - -c
        - |
          while true; do
            for i in $(seq 1 1000); do
              wget -q -O- http://nginx-app-service.production.svc.cluster.local/ > /dev/null &
            done
            wait
            echo "Load test cycle completed"
            sleep 1
          done
```

### Autoscaling Commands
```bash
# Deploy HPA
kubectl apply -f nginx-hpa.yaml

# Check HPA status
kubectl get hpa -n production
kubectl describe hpa nginx-app-hpa -n production

# Watch HPA in action
kubectl get hpa nginx-app-hpa -n production --watch

# Check current metrics
kubectl top pods -n production
kubectl top nodes

# Create load to trigger scaling
kubectl apply -f load-test-deployment.yaml

# Monitor scaling
kubectl get pods -n production -l app=nginx-app --watch

# VPA commands
kubectl get vpa -n production
kubectl describe vpa nginx-app-vpa -n production

# Check VPA recommendations
kubectl get vpa nginx-app-vpa -n production -o yaml
```

---

## Monitoring & Logging - Observability

### **What**: Collect metrics, logs, and traces for system visibility
### **Why**: Troubleshooting, performance optimization, alerting
### **Tools**: Prometheus (metrics), Grafana (visualization), ELK/EFK (logs)

### Prometheus for Nginx Monitoring
```yaml
# prometheus-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
    
    rule_files:
    - "/etc/prometheus/rules/*.yml"
    
    alerting:
      alertmanagers:
      - static_configs:
        - targets:
          - alertmanager:9093
    
    scrape_configs:
    # Kubernetes API server
    - job_name: 'kubernetes-apiservers'
      kubernetes_sd_configs:
      - role: endpoints
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      relabel_configs:
      - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
        action: keep
        regex: default;kubernetes;https
    
    # Nginx application metrics
    - job_name: 'nginx-app'
      kubernetes_sd_configs:
      - role: pod
        namespaces:
          names:
          - production
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - source_labels: [__meta_kubernetes_namespace]
        action: replace
        target_label: kubernetes_namespace
      - source_labels: [__meta_kubernetes_pod_name]
        action: replace
        target_label: kubernetes_pod_name
    
    # Node exporter
    - job_name: 'node-exporter'
      kubernetes_sd_configs:
      - role: endpoints
      relabel_configs:
      - source_labels: [__meta_kubernetes_endpoints_name]
        regex: 'node-exporter'
        action: keep
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      containers:
      - name: prometheus
        image: prom/prometheus:v2.45.0
        args:
        - '--config.file=/etc/prometheus/prometheus.yml'
        - '--storage.tsdb.path=/prometheus/'
        - '--web.console.libraries=/etc/prometheus/console_libraries'
        - '--web.console.templates=/etc/prometheus/consoles'
        - '--storage.tsdb.retention.time=30d'
        - '--web.enable-lifecycle'
        - '--web.enable-admin-api'
        ports:
        - containerPort: 9090
        volumeMounts:
        - name: prometheus-config-volume
          mountPath: /etc/prometheus/
        - name: prometheus-storage-volume
          mountPath: /prometheus/
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
      volumes:
      - name: prometheus-config-volume
        configMap:
          name: prometheus-config
      - name: prometheus-storage-volume
        emptyDir: {}
```

### Grafana Dashboard for Nginx
```yaml
# grafana-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: grafana
  template:
    metadata:
      labels:
        app: grafana
    spec:
      containers:
      - name: grafana
        image: grafana/grafana:10.0.0
        ports:
        - containerPort: 3000
        env:
        - name: GF_SECURITY_ADMIN_PASSWORD
          valueFrom:
            secretKeyRef:
              name: grafana-secret
              key: admin-password
        volumeMounts:
        - name: grafana-storage
          mountPath: /var/lib/grafana
        - name: grafana-config
          mountPath: /etc/grafana/provisioning
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "200m"
      volumes:
      - name: grafana-storage
        emptyDir: {}
      - name: grafana-config
        configMap:
          name: grafana-config
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-config
  namespace: monitoring
data:
  datasource.yaml: |
    apiVersion: 1
    datasources:
    - name: Prometheus
      type: prometheus
      access: proxy
      url: http://prometheus:9090
      isDefault: true
  dashboard.json: |
    {
      "dashboard": {
        "title": "Nginx Application Dashboard",
        "panels": [
          {
            "title": "HTTP Requests per Second",
            "type": "graph",
            "targets": [
              {
                "expr": "rate(nginx_http_requests_total[5m])",
                "legendFormat": "{{instance}}"
              }
            ]
          },
          {
            "title": "Response Time",
            "type": "graph", 
            "targets": [
              {
                "expr": "nginx_http_request_duration_seconds",
                "legendFormat": "{{instance}}"
              }
            ]
          }
        ]
      }
    }
```

### EFK Stack for Logging
```yaml
# elasticsearch.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: elasticsearch
  namespace: logging
spec:
  serviceName: elasticsearch
  replicas: 1
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels:
        app: elasticsearch
    spec:
      containers:
      - name: elasticsearch
        image: docker.elastic.co/elasticsearch/elasticsearch:8.8.0
        env:
        - name: discovery.type
          value: "single-node"
        - name: ES_JAVA_OPTS
          value: "-Xms512m -Xmx512m"
        - name: xpack.security.enabled
          value: "false"
        ports:
        - containerPort: 9200
        - containerPort: 9300
        volumeMounts:
        - name: elasticsearch-data
          mountPath: /usr/share/elasticsearch/data
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1"
  volumeClaimTemplates:
  - metadata:
      name: elasticsearch-data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 10Gi
```

### Monitoring Commands
```bash
# Deploy monitoring stack
kubectl create namespace monitoring
kubectl apply -f prometheus-config.yaml
kubectl apply -f grafana-deployment.yaml

# Check monitoring pods
kubectl get pods -n monitoring
kubectl logs -f deployment/prometheus -n monitoring

# Access Grafana (port-forward)
kubectl port-forward service/grafana 3000:3000 -n monitoring
# Visit: http://localhost:3000 (admin/admin)

# Check metrics endpoint
kubectl port-forward service/prometheus 9090:9090 -n monitoring
# Visit: http://localhost:9090

# View nginx metrics
kubectl exec -it nginx-app-xxx -n production -- curl localhost:9113/metrics

# Check logs
kubectl logs -f deployment/nginx-app -n production
kubectl logs deployment/nginx-app -n production --tail=100 --timestamps

# Get events
kubectl get events -n production --sort-by=.metadata.creationTimestamp
```

---

## Security & RBAC - Access Control

### **What**: Authentication, authorization, and security policies
### **Why**: Protect cluster resources and enforce least privilege
### **Components**: ServiceAccounts, Roles, RoleBindings, NetworkPolicies

### ServiceAccount for Nginx App
```yaml
# nginx-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nginx-app-sa
  namespace: production
  annotations:
    # For AWS EKS with IAM roles
    eks.amazonaws.com/role-arn: arn:aws:iam::123456789012:role/NginxAppRole
automountServiceAccountToken: true
---
apiVersion: v1
kind: Secret
metadata:
  name: nginx-app-sa-token
  namespace: production
  annotations:
    kubernetes.io/service-account.name: nginx-app-sa
type: kubernetes.io/service-account-token
```

### RBAC for Nginx App
```yaml
# nginx-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: production
  name: nginx-app-role
rules:
# Allow reading configmaps and secrets
- apiGroups: [""]
  resources: ["configmaps", "secrets"]
  verbs: ["get", "list", "watch"]
# Allow reading own pod info
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list"]
  resourceNames: ["nginx-app*"]
# Allow updating own deployment (for self-healing)
- apiGroups: ["apps"]
  resources: ["deployments"]
  verbs: ["get", "patch"]
  resourceNames: ["nginx-app"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: nginx-app-binding
  namespace: production
subjects:
- kind: ServiceAccount
  name: nginx-app-sa
  namespace: production
roleRef:
  kind: Role
  name: nginx-app-role
  apiGroup: rbac.authorization.k8s.io
```

### Network Policy for Nginx
```yaml
# nginx-network-policy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: nginx-app-netpol
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: nginx-app
  policyTypes:
  - Ingress
  - Egress
  ingress:
  # Allow traffic from ingress controller
  - from:
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
    ports:
    - protocol: TCP
      port: 80
  # Allow traffic from monitoring
  - from:
    - namespaceSelector:
        matchLabels:
          name: monitoring
    ports:
    - protocol: TCP
      port: 9113
  # Allow traffic from load balancer health checks
  - from: []
    ports:
    - protocol: TCP
      port: 80
  egress:
  # Allow DNS resolution
  - to: []
    ports:
    - protocol: UDP
      port: 53
    - protocol: TCP
      port: 53
  # Allow HTTPS outbound (for API calls)
  - to: []
    ports:
    - protocol: TCP
      port: 443
  # Allow access to Kubernetes API
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: TCP
      port: 443
```

### Pod Security Standards
```yaml
# nginx-deployment-secure.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-app-secure
  namespace: production
  labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-app
  template:
    metadata:
      labels:
        app: nginx-app
    spec:
      serviceAccountName: nginx-app-sa
      securityContext:
        runAsNonRoot: true
        runAsUser: 101
        runAsGroup: 101
        fsGroup: 101
        seccompProfile:
          type: RuntimeDefault
      containers:
      - name: nginx
        image: your-dockerhub-username/nginx-app:latest
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL
            add:
            - NET_BIND_SERVICE
        ports:
        - containerPort: 8080  # Run on unprivileged port
        volumeMounts:
        - name: tmp
          mountPath: /tmp
        - name: var-cache
          mountPath: /var/cache/nginx
        - name: var-run
          mountPath: /var/run
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"
      volumes:
      - name: tmp
        emptyDir: {}
      - name: var-cache
        emptyDir: {}
      - name: var-run
        emptyDir: {}
```

### Security Commands
```bash
# Check RBAC
kubectl get roles -n production
kubectl get rolebindings -n production
kubectl describe role nginx-app-role -n production

# Test permissions
kubectl auth can-i get pods --as=system:serviceaccount:production:nginx-app-sa -n production
kubectl auth can-i create deployments --as=system:serviceaccount:production:nginx-app-sa -n production

# Check network policies
kubectl get networkpolicy -n production
kubectl describe networkpolicy nginx-app-netpol -n production

# Security scanning
kubectl get pods -n production -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.spec.securityContext}{"\n"}{end}'

# Check pod security
kubectl get pods nginx-app-xxx -n production -o yaml | grep -A 10 securityContext
```

---

## Advanced Patterns - Production Excellence

### **What**: Advanced deployment patterns and operational practices
### **Why**: Zero-downtime deployments, resilience, efficiency
### **Patterns**: Blue-Green, Canary, Circuit Breaker, Init Containers

### Blue-Green Deployment
```yaml
# nginx-blue-green.yaml
# Blue (current) deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-app-blue
  namespace: production
  labels:
    app: nginx-app
    version: blue
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-app
      version: blue
  template:
    metadata:
      labels:
        app: nginx-app
        version: blue
    spec:
      containers:
      - name: nginx
        image: your-dockerhub-username/nginx-app:v1.0
        ports:
        - containerPort: 80
---
# Green (new) deployment  
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-app-green
  namespace: production
  labels:
    app: nginx-app
    version: green
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-app
      version: green
  template:
    metadata:
      labels:
        app: nginx-app
        version: green
    spec:
      containers:
      - name: nginx
        image: your-dockerhub-username/nginx-app:v2.0
        ports:
        - containerPort: 80
---
# Service pointing to blue (switch to green when ready)
apiVersion: v1
kind: Service
metadata:
  name: nginx-app-service
  namespace: production
spec:
  selector:
    app: nginx-app
    version: blue  # Change to 'green' to switch
  ports:
  - port: 80
    targetPort: 80
```

### Canary Deployment with Istio
```yaml
# nginx-canary-istio.yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: nginx-app-vs
  namespace: production
spec:
  http:
  - match:
    - headers:
        canary:
          exact: "true"
    route:
    - destination:
        host: nginx-app-service
        subset: canary
  - route:
    - destination:
        host: nginx-app-service
        subset: stable
      weight: 90
    - destination:
        host: nginx-app-service
        subset: canary
      weight: 10
---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: nginx-app-dr
  namespace: production
spec:
  host: nginx-app-service
  subsets:
  - name: stable
    labels:
      version: stable
  - name: canary
    labels:
      version: canary
```

### Init Container Pattern
```yaml
# nginx-with-init.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-app-with-init
  namespace: production
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-app
  template:
    metadata:
      labels:
        app: nginx-app
    spec:
      initContainers:
      # Wait for database
      - name: wait-for-db
        image: busybox:1.35
        command: ['sh', '-c']
        args:
        - |
          echo "Waiting for database..."
          until nc -z postgres-service 5432; do
            echo "Database not ready, sleeping..."
            sleep 2
          done
          echo "Database is ready!"
      # Download configuration
      - name: config-downloader
        image: busybox:1.35
        command: ['sh', '-c']
        args:
        - |
          echo "Downloading configuration..."
          wget -O /shared/app-config.json http://config-service/api/config
          echo "Configuration downloaded"
        volumeMounts:
        - name: shared-config
          mountPath: /shared
      # Set up permissions
      - name: setup-permissions
        image: busybox:1.35
        command: ['sh', '-c']
        args:
        - |
          echo "Setting up permissions..."
          chown -R 101:101 /app/data
          chmod 755 /app/data
        volumeMounts:
        - name: app-data
          mountPath: /app/data
      containers:
      - name: nginx
        image: your-dockerhub-username/nginx-app:latest
        volumeMounts:
        - name: shared-config
          mountPath: /etc/app
        - name: app-data
          mountPath: /app/data
      volumes:
      - name: shared-config
        emptyDir: {}
      - name: app-data
        emptyDir: {}
```

### Multi-Container Pod with Sidecar
```yaml
# nginx-with-sidecar.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-app-with-sidecar
  namespace: production
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-app
  template:
    metadata:
      labels:
        app: nginx-app
    spec:
      containers:
      # Main application
      - name: nginx
        image: your-dockerhub-username/nginx-app:latest
        ports:
        - containerPort: 80
        volumeMounts:
        - name: shared-logs
          mountPath: /var/log/nginx
      # Log processor sidecar
      - name: log-processor
        image: busybox:1.35
        command: ["/bin/sh"]
        args:
        - -c
        - |
          while true; do
            tail -F /var/log/nginx/access.log | while read line; do
              echo "$(date '+%Y-%m-%d %H:%M:%S') [LOG-PROCESSOR] $line"
            done
          done
        volumeMounts:
        - name: shared-logs
          mountPath: /var/log/nginx
      # Metrics exporter sidecar
      - name: metrics-exporter
        image: nginx/nginx-prometheus-exporter:0.11.0
        args:
        - -nginx.scrape-uri=http://localhost:80/nginx_status
        ports:
        - containerPort: 9113
          name: metrics
      volumes:
      - name: shared-logs
        emptyDir: {}
```

### Resource Quotas and Limits
```yaml
# resource-quota.yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: nginx-app-quota
  namespace: production
spec:
  hard:
    requests.cpu: "4"
    requests.memory: 8Gi
    limits.cpu: "8"
    limits.memory: 16Gi
    persistentvolumeclaims: "10"
    pods: "20"
    services: "5"
    secrets: "10"
    configmaps: "10"
---
apiVersion: v1
kind: LimitRange
metadata:
  name: nginx-app-limits
  namespace: production
spec:
  limits:
  - default:
      cpu: "200m"
      memory: "256Mi"
    defaultRequest:
      cpu: "100m"
      memory: "128Mi"
    type: Container
  - max:
      cpu: "1"
      memory: "1Gi"
    min:
      cpu: "50m"
      memory: "64Mi"
    type: Container
```

---

## AWS EKS - Managed Kubernetes

### **What**: Amazon's managed Kubernetes service
### **Why**: Reduced operational overhead, AWS integration
### **Features**: Managed control plane, IAM integration, auto-scaling

### EKS Cluster Setup with eksctl
```bash
# Install eksctl
curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
sudo mv /tmp/eksctl /usr/local/bin

# Create EKS cluster
eksctl create cluster \
  --name nginx-app-cluster \
  --version 1.28 \
  --region us-west-2 \
  --nodegroup-name nginx-workers \
  --node-type m5.large \
  --nodes 3 \
  --nodes-min 1 \
  --nodes-max 5 \
  --ssh-access \
  --ssh-public-key my-key \
  --managed
```

### EKS Cluster Configuration (YAML)
```yaml
# eks-cluster.yaml
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: nginx-app-cluster
  region: us-west-2
  version: "1.28"

iam:
  withOIDC: true
  serviceAccounts:
  - metadata:
      name: nginx-app-sa
      namespace: production
    roleName: NginxAppRole
    roleOnly: true
    attachPolicyARNs:
    - arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess
    - arn:aws:iam::aws:policy/SecretsManagerReadWrite

managedNodeGroups:
- name: nginx-workers
  instanceType: m5.large
  desiredCapacity: 3
  minSize: 1
  maxSize: 5
  volumeSize: 50
  ssh:
    enableSsm: true
  labels:
    role: worker
    environment: production
  tags:
    Environment: production
    Application: nginx-app
  iam:
    attachPolicyARNs:
    - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
    - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
    - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly

addons:
- name: vpc-cni
  version: latest
- name: coredns
  version: latest
- name: kube-proxy
  version: latest
- name: aws-ebs-csi-driver
  version: latest

cloudWatch:
  clusterLogging:
    enable: ["audit", "authenticator", "controllerManager"]
```

### AWS Load Balancer Controller
```yaml
# aws-load-balancer-controller.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/name: aws-load-balancer-controller
  name: aws-load-balancer-controller
  namespace: kube-system
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::123456789012:role/AmazonEKSLoadBalancerControllerRole
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/name: aws-load-balancer-controller
  name: aws-load-balancer-controller
  namespace: kube-system
spec:
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/name: aws-load-balancer-controller
  template:
    metadata:
      labels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/name: aws-load-balancer-controller
    spec:
      containers:
      - args:
        - --cluster-name=nginx-app-cluster
        - --ingress-class=alb
        image: amazon/aws-load-balancer-controller:v2.6.0
        name: controller
        resources:
          limits:
            cpu: 200m
            memory: 500Mi
          requests:
            cpu: 100m
            memory: 200Mi
      serviceAccountName: aws-load-balancer-controller
```

### EKS-specific Nginx Deployment
```yaml
# nginx-deployment-eks.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-app-eks
  namespace: production
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-app
  template:
    metadata:
      labels:
        app: nginx-app
    spec:
      serviceAccountName: nginx-app-sa
      containers:
      - name: nginx
        image: your-dockerhub-username/nginx-app:latest
        ports:
        - containerPort: 80
        env:
        - name: AWS_REGION
          value: us-west-2
        - name: CLUSTER_NAME
          value: nginx-app-cluster
        volumeMounts:
        - name: aws-iam-token
          mountPath: /var/run/secrets/eks.amazonaws.com/serviceaccount
          readOnly: true
      volumes:
      - name: aws-iam-token
        projected:
          sources:
          - serviceAccountToken:
              audience: sts.amazonaws.com
              expirationSeconds: 86400
              path: token
      nodeSelector:
        kubernetes.io/arch: amd64
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-app-service-eks
  namespace: production
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: nlb
    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: http
    service.beta.kubernetes.io/aws-load-balancer-healthcheck-path: "/health"
spec:
  type: LoadBalancer
  selector:
    app: nginx-app
  ports:
  - port: 80
    targetPort: 80
```

### EKS Commands
```bash
# Create cluster
eksctl create cluster -f eks-cluster.yaml

# Update kubeconfig
aws eks update-kubeconfig --region us-west-2 --name nginx-app-cluster

# Check cluster
kubectl get nodes
kubectl get pods -n kube-system

# Deploy to EKS
kubectl apply -f nginx-deployment-eks.yaml

# Check AWS Load Balancer
kubectl get service nginx-app-service-eks -n production

# Scale node group
eksctl scale nodegroup --cluster=nginx-app-cluster --nodes=5 nginx-workers

# Delete cluster (be careful!)
eksctl delete cluster nginx-app-cluster
```

---

## Helm & Templating - Package Management

### **What**: Kubernetes package manager and templating engine
### **Why**: Reusable, configurable, versioned application deployments
### **Features**: Charts, templates, values, releases

### Helm Chart Structure for Nginx App
```bash
# Create Helm chart
helm create nginx-app-chart
cd nginx-app-chart

# Chart structure:
# nginx-app-chart/
# ├── Chart.yaml
# ├── values.yaml
# ├── templates/
# │   ├── deployment.yaml
# │   ├── service.yaml
# │   ├── ingress.yaml
# │   ├── configmap.yaml
# │   └── _helpers.tpl
# └── charts/
```

### Chart.yaml
```yaml
# Chart.yaml
apiVersion: v2
name: nginx-app
description: A Helm chart for Nginx application
type: application
version: 0.1.0
appVersion: "1.0.0"
keywords:
  - nginx
  - web
  - http
home: https://github.com/yourusername/nginx-app
sources:
  - https://github.com/yourusername/nginx-app
maintainers:
  - name: Your Name
    email: your.email@example.com
```

### values.yaml
```yaml
# values.yaml
replicaCount: 3

image:
  repository: your-dockerhub-username/nginx-app
  pullPolicy: IfNotPresent
  tag: "latest"

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  create: true
  annotations: {}
  name: ""

podAnnotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "9113"

podSecurityContext:
  runAsNonRoot: true
  runAsUser: 101
  fsGroup: 101

securityContext:
  allowPrivilegeEscalation: false
  readOnlyRootFilesystem: true
  capabilities:
    drop:
    - ALL
    add:
    - NET_BIND_SERVICE

service:
  type: ClusterIP
  port: 80
  targetPort: 8080

ingress:
  enabled: true
  className: "nginx"
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    nginx.ingress.kubernetes.io/rate-limit: "100"
  hosts:
    - host: nginx-app.example.com
      paths:
        - path: /
          pathType: Prefix
  tls:
    - secretName: nginx-app-tls
      hosts:
        - nginx-app.example.com

resources:
  limits:
    cpu: 500m
    memory: 512Mi
  requests:
    cpu: 250m
    memory: 256Mi

autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 20
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

nodeSelector: {}
tolerations: []
affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchExpressions:
          - key: app.kubernetes.io/name
            operator: In
            values:
            - nginx-app
        topologyKey: kubernetes.io/hostname

config:
  nginx:
    workerConnections: 1024
    keepaliveTimeout: 65
  app:
    environment: production
    logLevel: info

persistence:
  enabled: true
  storageClass: "fast-ssd"
  size: 5Gi

monitoring:
  enabled: true
  prometheusExporter:
    image: nginx/nginx-prometheus-exporter:0.11.0
    port: 9113
```

### templates/deployment.yaml
```yaml
# templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "nginx-app.fullname" . }}
  labels:
    {{- include "nginx-app.labels" . | nindent 4 }}
spec:
  {{- if not .Values.autoscaling.enabled }}
  replicas: {{ .Values.replicaCount }}
  {{- end }}
  selector:
    matchLabels:
      {{- include "nginx-app.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      annotations:
        {{- with .Values.podAnnotations }}
        {{- toYaml . | nindent 8 }}
        {{- end }}
        checksum/config: {{ include (print $.Template.BasePath "/configmap.yaml") . | sha256sum }}
      labels:
        {{- include "nginx-app.selectorLabels" . | nindent 8 }}
    spec:
      {{- with .Values.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      serviceAccountName: {{ include "nginx-app.serviceAccountName" . }}
      securityContext:
        {{- toYaml .Values.podSecurityContext | nindent 8 }}
      containers:
        - name: {{ .Chart.Name }}
          securityContext:
            {{- toYaml .Values.securityContext | nindent 12 }}
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          ports:
            - name: http
              containerPort: {{ .Values.service.targetPort }}
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 30
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /ready
              port: http
            initialDelaySeconds: 5
            periodSeconds: 5
          resources:
            {{- toYaml .Values.resources | nindent 12 }}
          volumeMounts:
            - name: config
              mountPath: /etc/nginx/nginx.conf
              subPath: nginx.conf
            {{- if .Values.persistence.enabled }}
            - name: data
              mountPath: /var/log/nginx
            {{- end }}
        {{- if .Values.monitoring.enabled }}
        - name: nginx-exporter
          image: {{ .Values.monitoring.prometheusExporter.image }}
          args:
            - -nginx.scrape-uri=http://localhost:{{ .Values.service.targetPort }}/nginx_status
          ports:
            - name: metrics
              containerPort: {{ .Values.monitoring.prometheusExporter.port }}
        {{- end }}
      volumes:
        - name: config
          configMap:
            name: {{ include "nginx-app.fullname" . }}-config
        {{- if .Values.persistence.enabled }}
        - name: data
          persistentVolumeClaim:
            claimName: {{ include "nginx-app.fullname" . }}-data
        {{- end }}
      {{- with .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
```

### Environment-specific Values
```yaml
# values-dev.yaml
replicaCount: 1
image:
  tag: "dev"
config:
  app:
    environment: development
    logLevel: debug
ingress:
  hosts:
    - host: nginx-app-dev.example.com

# values-staging.yaml  
replicaCount: 2
image:
  tag: "staging"
config:
  app:
    environment: staging
ingress:
  hosts:
    - host: nginx-app-staging.example.com

# values-prod.yaml
replicaCount: 5
image:
  tag: "v1.0.0"
config:
  app:
    environment: production
resources:
  limits:
    cpu: 1
    memory: 1Gi
  requests:
    cpu: 500m
    memory: 512Mi
```

### Helm Commands
```bash
# Install Helm
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

# Create chart
helm create nginx-app-chart

# Validate chart
helm lint nginx-app-chart
helm template nginx-app-chart --debug

# Install release
helm install nginx-app nginx-app-chart -n production --create-namespace

# Install with custom values
helm install nginx-app-dev nginx-app-chart -f values-dev.yaml -n development --create-namespace

# Upgrade release
helm upgrade nginx-app nginx-app-chart -n production

# Check releases
helm list -A
helm status nginx-app -n production

# Rollback
helm rollback nginx-app 1 -n production

# Uninstall
helm uninstall nginx-app -n production

# Package chart
helm package nginx-app-chart

# Add to repository
helm repo add my-repo https://charts.example.com
helm repo update
```

---

## Troubleshooting - Production Issues

### **What**: Diagnose and resolve Kubernetes issues
### **Why**: Minimize downtime, maintain service quality
### **Areas**: Pods, networking, storage, performance, security

### Common Issues and Solutions

#### Pod Issues
```bash
# Pod stuck in Pending
kubectl describe pod <pod-name> -n production
kubectl get events -n production --field-selector involvedObject.name=<pod-name>

# Check node resources
kubectl top nodes
kubectl describe node <node-name>

# Pod stuck in CrashLoopBackOff
kubectl logs <pod-name> -n production --previous
kubectl describe pod <pod-name> -n production

# Debug with temporary pod
kubectl debug <pod-name> -n production -it --image=busybox --target=nginx
```

#### Networking Issues
```bash
# Service not reachable
kubectl get svc -n production
kubectl get endpoints -n production
kubectl describe service nginx-app-service -n production

# Test service connectivity
kubectl run test-pod --image=busybox -n production -it --rm -- sh
# Inside pod: wget -qO- nginx-app-service.production.svc.cluster.local

# DNS issues
kubectl run debug-dns --image=busybox -n production -it --rm -- nslookup kubernetes.default
kubectl logs -n kube-system deployment/coredns

# Network policy troubleshooting
kubectl get networkpolicy -n production
kubectl describe networkpolicy nginx-app-netpol -n production
```

#### Storage Issues
```bash
# PVC stuck in Pending
kubectl describe pvc nginx-logs-pvc -n production
kubectl get pv
kubectl get storageclass

# Mount issues
kubectl describe pod <pod-name> -n production | grep -A 10 -B 10 Mount
kubectl exec <pod-name> -n production -- df -h
```

#### Performance Issues
```bash
# Resource monitoring
kubectl top pods -n production --sort-by=memory
kubectl top nodes

# Check resource limits
kubectl get pods -n production -o custom-columns=NAME:.metadata.name,REQUESTS:.spec.containers[*].resources.requests,LIMITS:.spec.containers[*].resources.limits

# Application metrics
kubectl port-forward service/nginx-app-service 9113:9113 -n production
curl http://localhost:9113/metrics
```

### Troubleshooting Toolkit
```yaml
# troubleshooting-toolkit.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: debug-toolkit
  namespace: troubleshooting
spec:
  replicas: 1
  selector:
    matchLabels:
      app: debug-toolkit
  template:
    metadata:
      labels:
        app: debug-toolkit
    spec:
      containers:
      - name: toolkit
        image: nicolaka/netshoot
        command: ["/bin/bash"]
        args: ["-c", "sleep 3600"]
        securityContext:
          capabilities:
            add: ["NET_ADMIN", "SYS_PTRACE"]
        volumeMounts:
        - name: docker-sock
          mountPath: /var/run/docker.sock
        - name: proc
          mountPath: /host/proc
          readOnly: true
        - name: sys
          mountPath: /host/sys
          readOnly: true
      volumes:
      - name: docker-sock
        hostPath:
          path: /var/run/docker.sock
      - name: proc
        hostPath:
          path: /proc
      - name: sys
        hostPath:
          path: /sys
      hostNetwork: true
      hostPID: true
```

### Emergency Commands
```bash
# Force delete stuck pod
kubectl delete pod <pod-name> -n production --force --grace-period=0

# Drain node for maintenance
kubectl drain <node-name> --ignore-daemonsets --delete-emptydir-data

# Uncordon node after maintenance
kubectl uncordon <node-name>

# Emergency scale down
kubectl scale deployment nginx-app --replicas=0 -n production

# Get cluster events
kubectl get events --all-namespaces --sort-by=.metadata.creationTimestamp

# Check cluster health
kubectl get componentstatuses
kubectl cluster-info
kubectl get nodes

# Emergency backup
kubectl get all --all-namespaces -o yaml > emergency-backup.yaml

# Check API server health
kubectl get --raw='/healthz'
kubectl get --raw='/readyz'

# Memory and disk usage on nodes
kubectl get nodes -o custom-columns=NAME:.metadata.name,MEMORY:.status.allocatable.memory,CPU:.status.allocatable.cpu

# Restart deployment (last resort)
kubectl rollout restart deployment/nginx-app -n production
```

### Log Analysis Commands
```bash
# Comprehensive log collection
kubectl logs -n production deployment/nginx-app --all-containers --timestamps --since=1h > nginx-logs.txt

# Search for errors across all pods
kubectl logs -n production -l app=nginx-app | grep -i error

# Monitor logs in real-time
kubectl logs -n production -l app=nginx-app -f --max-log-requests=10

# System component logs
kubectl logs -n kube-system deployment/coredns
kubectl logs -n kube-system daemonset/kube-proxy
```

### Performance Debugging
```bash
# Check resource usage trends
kubectl top pods -n production --containers
kubectl top nodes --sort-by=memory

# Analyze slow-performing pods
kubectl describe pod <slow-pod> -n production | grep -A 5 -B 5 "Resource\|Limit\|Request"

# Network latency testing
kubectl exec -it debug-toolkit -n troubleshooting -- ping nginx-app-service.production.svc.cluster.local
kubectl exec -it debug-toolkit -n troubleshooting -- traceroute nginx-app-service.production.svc.cluster.local
```

---

## 1-Week Kubernetes Mastery Plan

### **Day 1: Foundation & Basic Deployment**
**Goal**: Deploy your nginx image as a basic pod and service

**Morning (2-3 hours):**
- Set up local cluster (minikube/kind) or cloud cluster
- Understand cluster architecture (master/worker nodes)
- Learn essential kubectl commands

**Afternoon (2-3 hours):**
- Create your first pod with nginx image
- Deploy as Deployment with 3 replicas
- Create ClusterIP service
- Test connectivity with port-forward

**Practice Project:**
```bash
# Deploy your nginx app
kubectl create deployment nginx-app --image=your-dockerhub-username/nginx-app:latest
kubectl scale deployment nginx-app --replicas=3
kubectl expose deployment nginx-app --port=80 --target-port=80
kubectl port-forward service/nginx-app 8080:80
```

### **Day 2: Configuration & Storage**
**Goal**: Add persistent storage and external configuration

**Morning (2-3 hours):**
- Create ConfigMaps for nginx configuration
- Use Secrets for sensitive data
- Mount configurations in pods

**Afternoon (2-3 hours):**
- Set up persistent volumes for logs
- Configure storage classes
- Deploy StatefulSet for persistent nginx

**Practice Project:**
```bash
# Add persistent storage and config
kubectl create configmap nginx-config --from-file=nginx.conf
kubectl create secret generic nginx-secrets --from-literal=password=secret123
# Apply deployment with mounted config and storage
```

### **Day 3: Networking & External Access**
**Goal**: Expose your application to external traffic

**Morning (2-3 hours):**
- Set up Ingress controller
- Create Ingress rules for your nginx app
- Configure SSL/TLS termination

**Afternoon (2-3 hours):**
- Implement Network Policies
- Test service discovery
- Explore Gateway API (bonus)

**Practice Project:**
```bash
# External access setup
kubectl apply -f nginx-ingress-controller.yaml
kubectl apply -f nginx-ingress.yaml
# Test: curl -H "Host: nginx-app.local" http://<ingress-ip>/
```

### **Day 4: Auto-scaling & Monitoring**
**Goal**: Make your application production-ready with scaling and observability

**Morning (2-3 hours):**
- Deploy Horizontal Pod Autoscaler
- Set up resource quotas and limits
- Configure Pod Disruption Budgets

**Afternoon (2-3 hours):**
- Deploy Prometheus for metrics
- Set up basic Grafana dashboard
- Configure log aggregation

**Practice Project:**
```bash
# Production readiness
kubectl apply -f nginx-hpa.yaml
kubectl apply -f prometheus-config.yaml
# Load test and watch auto-scaling
```

### **Day 5: Security & RBAC**
**Goal**: Secure your application and implement access controls

**Morning (2-3 hours):**
- Create ServiceAccounts and RBAC rules
- Implement Pod Security Standards
- Configure Network Policies

**Afternoon (2-3 hours):**
- Set up secrets management
- Implement security scanning
- Practice security troubleshooting

**Practice Project:**
```bash
# Security implementation
kubectl apply -f nginx-rbac.yaml
kubectl apply -f nginx-network-policy.yaml
# Test RBAC: kubectl auth can-i get pods --as=system:serviceaccount:production:nginx-app-sa
```

### **Day 6: Advanced Patterns & Cloud (AWS EKS)**
**Goal**: Deploy to managed Kubernetes and implement advanced patterns

**Morning (2-3 hours):**
- Set up AWS EKS cluster
- Deploy nginx app to EKS
- Configure AWS Load Balancer Controller

**Afternoon (2-3 hours):**
- Implement Blue-Green deployment
- Try Canary deployment pattern
- Set up multi-container pods with sidecars

**Practice Project:**
```bash
# Cloud deployment
eksctl create cluster -f eks-cluster.yaml
kubectl apply -f nginx-deployment-eks.yaml
# Test: kubectl get service nginx-app-service-eks
```

### **Day 7: Package Management & Production Operations**
**Goal**: Create reusable deployments and master troubleshooting

**Morning (2-3 hours):**
- Create Helm chart for your nginx app
- Deploy with different environments (dev/staging/prod)
- Practice Helm operations

**Afternoon (2-3 hours):**
- Master troubleshooting techniques
- Practice emergency scenarios
- Create operational runbooks

**Practice Project:**
```bash
# Helm mastery
helm create nginx-app-chart
helm install nginx-app nginx-app-chart -f values-prod.yaml
# Practice: Break something and fix it!
```

---

## Quick Reference & Cheat Sheet

### **Essential Daily Commands**
```bash
# Status checks
kubectl get pods,svc,ing -o wide
kubectl top nodes && kubectl top pods
kubectl get events --sort-by=.metadata.creationTimestamp | tail -10

# Quick deployment
kubectl create deployment app --image=image:tag
kubectl expose deployment app --port=80
kubectl scale deployment app --replicas=3

# Debugging
kubectl logs -f deployment/app
kubectl describe pod <pod-name>
kubectl exec -it <pod-name> -- /bin/bash

# Updates
kubectl set image deployment/app container=image:new-tag
kubectl rollout status deployment/app
kubectl rollout undo deployment/app
```

### **Production Checklist**
- [ ] Resource requests and limits set
- [ ] Health checks configured (liveness/readiness)
- [ ] Security context applied (non-root user)
- [ ] Secrets used for sensitive data
- [ ] Network policies implemented
- [ ] RBAC configured
- [ ] Monitoring and logging enabled
- [ ] Auto-scaling configured
- [ ] Backup strategy in place
- [ ] Disaster recovery tested

### **Emergency Response Kit**
```bash
# Quick cluster health check
kubectl get nodes && kubectl get pods --all-namespaces | grep -v Running

# Resource pressure check  
kubectl top nodes && kubectl describe nodes | grep -A 5 "Resource\|Pressure"

# Restart failing deployment
kubectl rollout restart deployment/<deployment-name> -n <namespace>

# Scale down problematic app
kubectl scale deployment <deployment-name> --replicas=0 -n <namespace>

# Emergency pod for debugging
kubectl run debug --image=busybox -it --rm -- sh
```

---

## Congratulations! 🎉

You now have a comprehensive understanding of Kubernetes from basic concepts to production operations. Your nginx application has traveled through:

1. **Basic Pod** → **Production Deployment**
2. **Local Testing** → **Cloud Deployment (EKS)**  
3. **Manual Configuration** → **Helm Templates**
4. **Single Instance** → **Auto-scaled, Monitored, Secured Application**

### **Next Steps for Continued Learning:**

1. **Explore Service Mesh**: Implement Istio or Linkerd
2. **Advanced Networking**: Study CNI plugins, multi-cluster networking
3. **GitOps**: Set up ArgoCD or Flux for automated deployments
4. **Custom Resources**: Build your own Kubernetes operators
5. **Multi-cloud**: Deploy across AWS, GCP, and Azure
6. **Edge Computing**: Explore K3s and edge deployments

### **Key Takeaways:**

- **Start Simple**: Begin with basic pods, gradually add complexity
- **Use Your Own Images**: Nothing beats hands-on practice with real applications
- **Automate Everything**: From deployments to monitoring to scaling
- **Security First**: Implement security from day one, not as an afterthought
- **Monitor Constantly**: You can't improve what you don't measure
- **Practice Failure**: Break things in a safe environment to learn recovery

Remember: Kubernetes mastery comes from consistent practice and real-world experience. Keep deploying, keep learning, and keep building! 🚀

**Pro Tip**: Bookmark this guide and return to specific sections as you encounter new challenges in your Kubernetes journey. Each concept builds upon the previous ones, creating a solid foundation for production Kubernetes expertise.
