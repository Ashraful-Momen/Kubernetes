# Kubernetes Master Guide for Docker/Swarm Developers

## Table of Contents
1. [Kubernetes vs Docker Swarm - Key Differences](#k8s-vs-swarm)
2. [Kubernetes Architecture](#architecture)
3. [Cluster Setup and Management](#cluster-setup)
4. [Core Kubernetes Objects](#core-objects)
5. [Workload Management](#workloads)
6. [Networking and Service Discovery](#networking)
7. [Storage and Data Management](#storage)
8. [Configuration Management](#config-management)
9. [Security and RBAC](#security)
10. [Monitoring and Observability](#monitoring)
11. [CI/CD Integration](#cicd)
12. [Production Best Practices](#best-practices)
13. [Troubleshooting](#troubleshooting)
14. [Migration from Docker Swarm](#migration)
15. [Advanced Patterns and Tools](#advanced)

---

## 1. Kubernetes vs Docker Swarm - Key Differences {#k8s-vs-swarm}

### Conceptual Mapping

| Docker Swarm | Kubernetes | Key Differences |
|--------------|------------|----------------|
| Service | Deployment + Service | K8s separates app definition from network exposure |
| Task | Pod | Pod can contain multiple containers |
| Stack | Multiple K8s objects | More granular resource management |
| Node | Node | Similar concept |
| Overlay Network | CNI Plugins | More network options and flexibility |
| Secret/Config | Secret/ConfigMap | Similar but more advanced |

### Architecture Complexity
```
Docker Swarm: Simple, Docker-native
├── Manager Nodes (Raft consensus)
└── Worker Nodes (run containers)

Kubernetes: Complex, extensive ecosystem
├── Control Plane
│   ├── API Server (REST API)
│   ├── etcd (distributed key-value store)
│   ├── Controller Manager (desired state)
│   └── Scheduler (pod placement)
└── Worker Nodes
    ├── kubelet (node agent)
    ├── kube-proxy (network proxy)
    └── Container Runtime (Docker/containerd/CRI-O)
```

### Why Choose Kubernetes Over Swarm?
- **Ecosystem**: Vast third-party tools (Helm, Istio, Prometheus)
- **Flexibility**: Multiple deployment strategies and custom resources
- **Scalability**: Better for large, complex applications
- **Community**: Larger community and enterprise support
- **Features**: Auto-scaling, advanced networking, operators

---

## 2. Kubernetes Architecture {#architecture}

### Control Plane Components

```yaml
# Control Plane Overview
API Server:
  - REST API gateway
  - Authentication/Authorization
  - Admission controllers
  - Watches and notifications

etcd:
  - Distributed key-value store
  - Cluster state and configuration
  - Backup and restore critical

Controller Manager:
  - Deployment Controller
  - ReplicaSet Controller  
  - Service Controller
  - Node Controller

Scheduler:
  - Pod placement decisions
  - Resource requirements
  - Node affinity/anti-affinity
  - Taints and tolerations
```

### Worker Node Components

```yaml
# Worker Node Overview
kubelet:
  - Pod lifecycle management
  - Health checks and reporting
  - Volume mounting
  - Container runtime communication

kube-proxy:
  - Network proxy
  - Service discovery
  - Load balancing
  - iptables/IPVS rules

Container Runtime:
  - Docker (deprecated in 1.24+)
  - containerd (recommended)
  - CRI-O
```

### Kubernetes Objects Hierarchy

```
Cluster
├── Namespaces (logical isolation)
│   ├── Deployments (desired state for Pods)
│   │   └── ReplicaSets (Pod replicas)
│   │       └── Pods (container groups)
│   ├── Services (network exposure)
│   ├── ConfigMaps/Secrets (configuration)
│   ├── PersistentVolumes (storage)
│   └── Ingress (HTTP routing)
└── ClusterRoles/ClusterRoleBindings (cluster-wide RBAC)
```

---

## 3. Cluster Setup and Management {#cluster-setup}

### Production Cluster Options

```bash
# 1. Managed Services (Recommended for Production)
# AWS EKS
eksctl create cluster --name my-cluster --region us-west-2

# Google GKE  
gcloud container clusters create my-cluster

# Azure AKS
az aks create --resource-group myRG --name my-cluster

# 2. Self-Managed Options
# kubeadm (bare metal/VMs)
# kops (AWS)
# Rancher (multi-cloud)
# OpenShift (enterprise)
```

### Local Development Setup

```bash
# Option 1: minikube (single node)
minikube start --driver=docker --cpus=4 --memory=8g

# Option 2: kind (Kubernetes in Docker)
kind create cluster --config=kind-config.yaml

# Option 3: k3s (lightweight)
curl -sfL https://get.k3s.io | sh -

# Option 4: Docker Desktop (easiest)
# Enable Kubernetes in Docker Desktop settings
```

### Kind Configuration for Multi-Node

```yaml
# kind-config.yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  kubeadmConfigPatches:
  - |
    kind: InitConfiguration
    nodeRegistration:
      kubeletExtraArgs:
        node-labels: "ingress-ready=true"
  extraPortMappings:
  - containerPort: 80
    hostPort: 80
    protocol: TCP
  - containerPort: 443
    hostPort: 443
    protocol: TCP
- role: worker
- role: worker
- role: worker
```

### Essential Tools Setup

```bash
# kubectl (Kubernetes CLI)
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl

# Helm (Package Manager)
curl https://get.helm.sh/helm-v3.12.0-linux-amd64.tar.gz | tar -xzO linux-amd64/helm > helm
sudo install helm /usr/local/bin/

# k9s (Terminal UI)
curl -sL https://github.com/derailed/k9s/releases/latest/download/k9s_Linux_amd64.tar.gz | tar -xzf - k9s
sudo install k9s /usr/local/bin/

# kubectx/kubens (Context switching)
sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx
sudo ln -s /opt/kubectx/kubectx /usr/local/bin/kubectx
sudo ln -s /opt/kubectx/kubens /usr/local/bin/kubens
```

---

## 4. Core Kubernetes Objects {#core-objects}

### Pods - Basic Building Block

```yaml
# Unlike Swarm tasks, Pods can have multiple containers
apiVersion: v1
kind: Pod
metadata:
  name: multi-container-pod
  labels:
    app: myapp
spec:
  containers:
  - name: web-server
    image: nginx:1.20
    ports:
    - containerPort: 80
    volumeMounts:
    - name: shared-data
      mountPath: /usr/share/nginx/html
  - name: content-generator
    image: busybox
    command: ['/bin/sh']
    args: ['-c', 'while true; do echo "Hello $(date)" > /data/index.html; sleep 30; done']
    volumeMounts:
    - name: shared-data
      mountPath: /data
  volumes:
  - name: shared-data
    emptyDir: {}
```

### ReplicaSets - Pod Replicas

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: web-replicaset
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
      - name: web
        image: nginx:1.20
        ports:
        - containerPort: 80
```

### Deployments - Swarm Services Equivalent

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-deployment
  namespace: production
spec:
  replicas: 5
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
        version: v1
    spec:
      containers:
      - name: web
        image: nginx:1.20
        ports:
        - containerPort: 80
        resources:
          limits:
            cpu: 500m
            memory: 512Mi
          requests:
            cpu: 250m
            memory: 256Mi
        livenessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 5
        env:
        - name: ENV
          value: "production"
        volumeMounts:
        - name: config
          mountPath: /etc/nginx/nginx.conf
          subPath: nginx.conf
      volumes:
      - name: config
        configMap:
          name: nginx-config
      nodeSelector:
        environment: production
      tolerations:
      - key: "dedicated"
        operator: "Equal"
        value: "web"
        effect: "NoSchedule"
```

### Services - Network Exposure

```yaml
# ClusterIP Service (internal)
apiVersion: v1
kind: Service
metadata:
  name: web-service-internal
spec:
  selector:
    app: web
  ports:
  - port: 80
    targetPort: 80
  type: ClusterIP

---
# NodePort Service (external access via node ports)
apiVersion: v1
kind: Service
metadata:
  name: web-service-nodeport
spec:
  selector:
    app: web
  ports:
  - port: 80
    targetPort: 80
    nodePort: 30080
  type: NodePort

---
# LoadBalancer Service (cloud provider load balancer)
apiVersion: v1
kind: Service
metadata:
  name: web-service-lb
spec:
  selector:
    app: web
  ports:
  - port: 80
    targetPort: 80
  type: LoadBalancer

---
# Headless Service (for StatefulSets)
apiVersion: v1
kind: Service
metadata:
  name: database-headless
spec:
  clusterIP: None
  selector:
    app: database
  ports:
  - port: 3306
    targetPort: 3306
```

---

## 5. Workload Management {#workloads}

### StatefulSets - For Stateful Applications

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql-statefulset
spec:
  serviceName: mysql-headless
  replicas: 3
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql:8.0
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-secret
              key: password
        ports:
        - containerPort: 3306
        volumeMounts:
        - name: mysql-storage
          mountPath: /var/lib/mysql
        - name: mysql-config
          mountPath: /etc/mysql/mysql.conf.d
      volumes:
      - name: mysql-config
        configMap:
          name: mysql-config
  volumeClaimTemplates:
  - metadata:
      name: mysql-storage
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 10Gi
```

### DaemonSets - One Pod Per Node

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-exporter
spec:
  selector:
    matchLabels:
      app: node-exporter
  template:
    metadata:
      labels:
        app: node-exporter
    spec:
      hostNetwork: true
      hostPID: true
      containers:
      - name: node-exporter
        image: prom/node-exporter:latest
        ports:
        - containerPort: 9100
        volumeMounts:
        - name: proc
          mountPath: /host/proc
          readOnly: true
        - name: sys
          mountPath: /host/sys
          readOnly: true
      volumes:
      - name: proc
        hostPath:
          path: /proc
      - name: sys
        hostPath:
          path: /sys
      tolerations:
      - effect: NoSchedule
        operator: Exists
```

### Jobs and CronJobs

```yaml
# Job (run to completion)
apiVersion: batch/v1
kind: Job
metadata:
  name: database-migration
spec:
  parallelism: 1
  completions: 1
  backoffLimit: 3
  template:
    spec:
      containers:
      - name: migrate
        image: migrate/migrate
        command: ['migrate']
        args: ['-path', '/migrations', '-database', 'postgres://user:pass@db:5432/mydb?sslmode=disable', 'up']
        volumeMounts:
        - name: migrations
          mountPath: /migrations
      volumes:
      - name: migrations
        configMap:
          name: database-migrations
      restartPolicy: OnFailure

---
# CronJob (scheduled)
apiVersion: batch/v1
kind: CronJob
metadata:
  name: backup-job
spec:
  schedule: "0 2 * * *"  # Daily at 2 AM
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: backup
            image: postgres:13
            command:
            - /bin/sh
            - -c
            - pg_dump -h $DB_HOST -U $DB_USER $DB_NAME > /backup/backup-$(date +%Y%m%d).sql
            env:
            - name: DB_HOST
              value: "postgres-service"
            - name: DB_USER
              value: "postgres"
            - name: PGPASSWORD
              valueFrom:
                secretKeyRef:
                  name: postgres-secret
                  key: password
            volumeMounts:
            - name: backup-storage
              mountPath: /backup
          volumes:
          - name: backup-storage
            persistentVolumeClaim:
              claimName: backup-pvc
          restartPolicy: OnFailure
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 1
```

---

## 6. Networking and Service Discovery {#networking}

### Ingress - HTTP/HTTPS Routing

```yaml
# First, install an Ingress Controller (nginx-ingress)
# helm install ingress-nginx ingress-nginx/ingress-nginx

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: app-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
spec:
  tls:
  - hosts:
    - myapp.example.com
    - api.example.com
    secretName: app-tls
  rules:
  - host: myapp.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: frontend-service
            port:
              number: 80
  - host: api.example.com
    http:
      paths:
      - path: /api
        pathType: Prefix
        backend:
          service:
            name: api-service
            port:
              number: 3000
      - path: /auth
        pathType: Prefix
        backend:
          service:
            name: auth-service
            port:
              number: 8080
```

### Network Policies - Micro-segmentation

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: api-network-policy
spec:
  podSelector:
    matchLabels:
      app: api
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: frontend
    - namespaceSelector:
        matchLabels:
          name: monitoring
    ports:
    - protocol: TCP
      port: 3000
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: database
    ports:
    - protocol: TCP
      port: 5432
  - to: []  # Allow DNS resolution
    ports:
    - protocol: UDP
      port: 53
```

### Service Mesh with Istio

```yaml
# Install Istio
# curl -L https://istio.io/downloadIstio | sh -
# istioctl install --set values.defaultRevision=default

# Enable sidecar injection
apiVersion: v1
kind: Namespace
metadata:
  name: production
  labels:
    istio-injection: enabled

---
# Virtual Service (routing rules)
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: api-vs
spec:
  hosts:
  - api.example.com
  http:
  - match:
    - headers:
        version:
          exact: v2
    route:
    - destination:
        host: api-service
        subset: v2
      weight: 100
  - route:
    - destination:
        host: api-service
        subset: v1
      weight: 80
    - destination:
        host: api-service
        subset: v2
      weight: 20

---
# Destination Rule (load balancing, circuit breaker)
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: api-dr
spec:
  host: api-service
  trafficPolicy:
    connectionPool:
      tcp:
        maxConnections: 10
      http:
        http1MaxPendingRequests: 10
        maxRequestsPerConnection: 2
    circuitBreaker:
      consecutiveErrors: 3
      interval: 30s
      baseEjectionTime: 30s
  subsets:
  - name: v1
    labels:
      version: v1
  - name: v2
    labels:
      version: v2
```

---

## 7. Storage and Data Management {#storage}

### Persistent Volumes and Claims

```yaml
# Storage Class (defines storage type)
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast-ssd
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp3
  iops: "3000"
  throughput: "125"
allowVolumeExpansion: true
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer

---
# Persistent Volume (manual provisioning)
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv
spec:
  capacity:
    storage: 100Gi
  accessModes:
  - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  storageClassName: nfs
  nfs:
    server: 192.168.1.100
    path: /data/shared

---
# Persistent Volume Claim (request for storage)
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: database-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi
  storageClassName: fast-ssd

---
# Using PVC in Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: database
spec:
  replicas: 1
  selector:
    matchLabels:
      app: database
  template:
    metadata:
      labels:
        app: database
    spec:
      containers:
      - name: postgres
        image: postgres:13
        env:
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres-secret
              key: password
        volumeMounts:
        - name: database-storage
          mountPath: /var/lib/postgresql/data
      volumes:
      - name: database-storage
        persistentVolumeClaim:
          claimName: database-pvc
```

### Volume Types Comparison

```yaml
# emptyDir (temporary storage)
volumes:
- name: temp-storage
  emptyDir:
    sizeLimit: 1Gi

# hostPath (node filesystem - avoid in production)
volumes:
- name: host-storage
  hostPath:
    path: /data
    type: DirectoryOrCreate

# configMap (configuration files)
volumes:
- name: config-volume
  configMap:
    name: app-config

# secret (sensitive data)
volumes:
- name: secret-volume
  secret:
    secretName: app-secret
    defaultMode: 0600

# projected (combine multiple sources)
volumes:
- name: combined-volume
  projected:
    sources:
    - configMap:
        name: app-config
    - secret:
        name: app-secret
```

---

## 8. Configuration Management {#config-management}

### ConfigMaps

```yaml
# ConfigMap from literal values
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  database.host: "postgres-service"
  database.port: "5432"
  redis.host: "redis-service"
  redis.port: "6379"
  app.env: "production"
  app.debug: "false"

---
# ConfigMap with file content
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-config
data:
  nginx.conf: |
    user nginx;
    worker_processes auto;
    error_log /var/log/nginx/error.log;
    pid /run/nginx.pid;

    events {
        worker_connections 1024;
    }

    http {
        log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                        '$status $body_bytes_sent "$http_referer" '
                        '"$http_user_agent" "$http_x_forwarded_for"';

        access_log /var/log/nginx/access.log main;

        sendfile on;
        tcp_nopush on;
        tcp_nodelay on;
        keepalive_timeout 65;
        types_hash_max_size 2048;

        include /etc/nginx/mime.types;
        default_type application/octet-stream;

        server {
            listen 80;
            server_name localhost;
            root /usr/share/nginx/html;
            index index.html index.htm;
        }
    }

---
# Using ConfigMap in Pod
apiVersion: v1
kind: Pod
metadata:
  name: app-pod
spec:
  containers:
  - name: app
    image: myapp:latest
    env:
    - name: DATABASE_HOST
      valueFrom:
        configMapKeyRef:
          name: app-config
          key: database.host
    - name: DATABASE_PORT
      valueFrom:
        configMapKeyRef:
          name: app-config
          key: database.port
    envFrom:
    - configMapRef:
        name: app-config
    volumeMounts:
    - name: config-volume
      mountPath: /etc/nginx/nginx.conf
      subPath: nginx.conf
  volumes:
  - name: config-volume
    configMap:
      name: nginx-config
```

### Secrets Management

```yaml
# Secret creation (base64 encoded)
apiVersion: v1
kind: Secret
metadata:
  name: app-secrets
type: Opaque
data:
  database-password: cGFzc3dvcmQxMjM=  # password123
  api-key: YWJjZGVmZ2hpams=  # abcdefghijk

---
# TLS Secret
apiVersion: v1
kind: Secret
metadata:
  name: tls-secret
type: kubernetes.io/tls
data:
  tls.crt: LS0tLS1CRUdJTi...  # base64 encoded certificate
  tls.key: LS0tLS1CRUdJTi...  # base64 encoded private key

---
# Docker Registry Secret
apiVersion: v1
kind: Secret
metadata:
  name: registry-secret
type: kubernetes.io/dockerconfigjson
data:
  .dockerconfigjson: ewogICJhdXRocyI6IHsKICAgICJyZWdpc3RyeS5leGFtcGxlLmNvbSI6IHsKICAgICAgInVzZXJuYW1lIjogInVzZXIiLAogICAgICAicGFzc3dvcmQiOiAicGFzcyIsCiAgICAgICJhdXRoIjogImRYTmxjam91Y0dGemN3PT0iCiAgICB9CiAgfQp9

---
# Using Secrets
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app
spec:
  template:
    spec:
      imagePullSecrets:
      - name: registry-secret
      containers:
      - name: app
        image: registry.example.com/myapp:latest
        env:
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: app-secrets
              key: database-password
        volumeMounts:
        - name: secret-volume
          mountPath: /etc/secrets
          readOnly: true
      volumes:
      - name: secret-volume
        secret:
          secretName: app-secrets
          defaultMode: 0600
```

### External Secrets Operator

```yaml
# Install External Secrets Operator
# helm repo add external-secrets https://charts.external-secrets.io
# helm install external-secrets external-secrets/external-secrets -n external-secrets-system --create-namespace

# AWS Secrets Manager Integration
apiVersion: external-secrets.io/v1beta1
kind: SecretStore
metadata:
  name: aws-secrets-manager
spec:
  provider:
    aws:
      service: SecretsManager
      region: us-west-2
      auth:
        serviceAccount:
          name: external-secrets-sa

---
apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: app-secret
spec:
  refreshInterval: 15m
  secretStoreRef:
    name: aws-secrets-manager
    kind: SecretStore
  target:
    name: app-secret
    creationPolicy: Owner
  data:
  - secretKey: database-password
    remoteRef:
      key: prod/database
      property: password
  - secretKey: api-key
    remoteRef:
      key: prod/api
      property: key
```

---

## 9. Security and RBAC {#security}

### Role-Based Access Control

```yaml
# Service Account
apiVersion: v1
kind: ServiceAccount
metadata:
  name: api-service-account
  namespace: production

---
# Role (namespace-scoped)
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: production
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods", "services"]
  verbs: ["get", "watch", "list"]
- apiGroups: ["apps"]
  resources: ["deployments"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]

---
# RoleBinding (bind role to service account)
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: production
subjects:
- kind: ServiceAccount
  name: api-service-account
  namespace: production
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io

---
# ClusterRole (cluster-scoped)
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: node-reader
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "list", "watch"]

---
# ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: read-nodes
subjects:
- kind: ServiceAccount
  name: monitoring-sa
  namespace: monitoring
roleRef:
  kind: ClusterRole
  name: node-reader
  apiGroup: rbac.authorization.k8s.io
```

### Pod Security Standards

```yaml
# Pod Security Policy (deprecated, use Pod Security Standards)
apiVersion: v1
kind: Namespace
metadata:
  name: secure-namespace
  labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted

---
# Security Context in Pod
apiVersion: v1
kind: Pod
metadata:
  name: secure-pod
spec:
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000
    seccompProfile:
      type: RuntimeDefault
  containers:
  - name: app
    image: myapp:latest
    securityContext:
      allowPrivilegeEscalation: false
      readOnlyRootFilesystem: true
      capabilities:
        drop:
        - ALL
        add:
        - NET_BIND_SERVICE
    volumeMounts:
    - name: tmp
      mountPath: /tmp
    - name: var-run
      mountPath: /var/run
  volumes:
  - name: tmp
    emptyDir: {}
  - name: var-run
    emptyDir: {}
```

### Network Security

```yaml
# Network Policy - Default Deny
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress

---
# Network Policy - Allow specific traffic
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-frontend-to-api
spec:
  podSelector:
    matchLabels:
      app: api
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: frontend
    ports:
    - protocol: TCP
      port: 3000
```

### Admission Controllers

```yaml
# Open Policy Agent (OPA) Gatekeeper
# Install: kubectl apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper/master/deploy/gatekeeper.yaml

# Constraint Template
apiVersion: templates.gatekeeper.sh/v1beta1
kind: ConstraintTemplate
metadata:
  name: k8srequiredlabels
spec:
  crd:
    spec:
      names:
        kind: K8sRequiredLabels
      validation:
        properties:
          labels:
            type: array
            items:
              type: string
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8srequiredlabels
        
        violation[{"msg": msg}] {
          required := input.parameters.labels
          provided := input.review.object.metadata.labels
          missing := required[_]
          not provided[missing]
          msg := sprintf("Missing required label: %v", [missing])
        }

---
# Constraint
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sRequiredLabels
metadata:
  name: must-have-environment
spec:
  match:
    kinds:
      - apiGroups: ["apps"]
        kinds: ["Deployment"]
  parameters:
    labels: ["environment", "team", "version"]
```

---

## 10. Monitoring and Observability {#monitoring}

### Prometheus Stack with Helm

```bash
# Add Prometheus community Helm repo
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

# Install kube-prometheus-stack (Prometheus + Grafana + AlertManager)
helm install monitoring prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --create-namespace \
  --set grafana.adminPassword=admin123 \
  --set prometheus.prometheusSpec.retention=30d \
  --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=50Gi
```

### Custom ServiceMonitor

```yaml
# ServiceMonitor for custom application
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: api-metrics
  namespace: monitoring
  labels:
    app: api
spec:
  selector:
    matchLabels:
      app: api
  endpoints:
  - port: metrics
    path: /metrics
    interval: 30s
    scrapeTimeout: 10s

---
# Service with metrics endpoint
apiVersion: v1
kind: Service
metadata:
  name: api-service
  labels:
    app: api
spec:
  selector:
    app: api
  ports:
  - name: http
    port: 3000
    targetPort: 3000
  - name: metrics
    port: 9090
    targetPort: 9090
```

### Application Monitoring Example

```yaml
# Deployment with monitoring
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: api
  template:
    metadata:
      labels:
        app: api
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9090"
        prometheus.io/path: "/metrics"
    spec:
      containers:
      - name: api
        image: myapi:latest
        ports:
        - containerPort: 3000
          name: http
        - containerPort: 9090
          name: metrics
        env:
        - name: METRICS_ENABLED
          value: "true"
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi
        livenessProbe:
          httpGet:
            path: /health
            port: 3000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 3000
          initialDelaySeconds: 5
          periodSeconds: 5
```

### Logging with ELK Stack

```yaml
# Elasticsearch
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: elasticsearch
spec:
  serviceName: elasticsearch
  replicas: 3
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels:
        app: elasticsearch
    spec:
      containers:
      - name: elasticsearch
        image: docker.elastic.co/elasticsearch/elasticsearch:8.8.0
        env:
        - name: discovery.type
          value: single-node
        - name: ES_JAVA_OPTS
          value: "-Xms1g -Xmx1g"
        ports:
        - containerPort: 9200
        volumeMounts:
        - name: elasticsearch-data
          mountPath: /usr/share/elasticsearch/data
  volumeClaimTemplates:
  - metadata:
      name: elasticsearch-data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 10Gi

---
# Fluentd DaemonSet
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd
spec:
  selector:
    matchLabels:
      app: fluentd
  template:
    metadata:
      labels:
        app: fluentd
    spec:
      containers:
      - name: fluentd
        image: fluent/fluentd-kubernetes-daemonset:v1-debian-elasticsearch
        env:
        - name: FLUENT_ELASTICSEARCH_HOST
          value: "elasticsearch-service"
        - name: FLUENT_ELASTICSEARCH_PORT
          value: "9200"
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: containers
          mountPath: /var/lib/docker/containers
        - name: fluentd-config
          mountPath: /fluentd/etc/kubernetes.conf
          subPath: kubernetes.conf
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: containers
        hostPath:
          path: /var/lib/docker/containers
      - name: fluentd-config
        configMap:
          name: fluentd-config
      tolerations:
      - effect: NoSchedule
        operator: Exists
```

### Distributed Tracing with Jaeger

```yaml
# Jaeger All-in-One (development)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jaeger
spec:
  replicas: 1
  selector:
    matchLabels:
      app: jaeger
  template:
    metadata:
      labels:
        app: jaeger
    spec:
      containers:
      - name: jaeger
        image: jaegertracing/all-in-one:latest
        env:
        - name: COLLECTOR_ZIPKIN_HOST_PORT
          value: ":9411"
        ports:
        - containerPort: 16686
        - containerPort: 14268
        - containerPort: 14250
        - containerPort: 9411

---
# Jaeger Service
apiVersion: v1
kind: Service
metadata:
  name: jaeger-service
spec:
  selector:
    app: jaeger
  ports:
  - name: ui
    port: 16686
    targetPort: 16686
  - name: collector-http
    port: 14268
    targetPort: 14268
  - name: collector-grpc
    port: 14250
    targetPort: 14250
```

---

## 11. CI/CD Integration {#cicd}

### GitLab CI/CD Pipeline

```yaml
# .gitlab-ci.yml
stages:
  - build
  - test
  - security
  - deploy-staging
  - deploy-production

variables:
  DOCKER_DRIVER: overlay2
  DOCKER_TLS_CERTDIR: "/certs"
  REGISTRY: $CI_REGISTRY
  IMAGE_NAME: $CI_REGISTRY_IMAGE
  KUBECONFIG: /tmp/kubeconfig

before_script:
  - echo $CI_REGISTRY_PASSWORD | docker login -u $CI_REGISTRY_USER --password-stdin $CI_REGISTRY

build:
  stage: build
  image: docker:20.10.16
  services:
    - docker:20.10.16-dind
  script:
    - docker build -t $IMAGE_NAME:$CI_COMMIT_SHA .
    - docker tag $IMAGE_NAME:$CI_COMMIT_SHA $IMAGE_NAME:latest
    - docker push $IMAGE_NAME:$CI_COMMIT_SHA
    - docker push $IMAGE_NAME:latest
  only:
    - main
    - develop

test:
  stage: test
  image: $IMAGE_NAME:$CI_COMMIT_SHA
  script:
    - npm test
    - npm run test:coverage
  coverage: '/Coverage: \d+.\d+%/'
  artifacts:
    reports:
      coverage_report:
        coverage_format: cobertura
        path: coverage/cobertura-coverage.xml

security-scan:
  stage: security
  image: docker:stable
  services:
    - docker:stable-dind
  script:
    - docker run --rm -v /var/run/docker.sock:/var/run/docker.sock 
      -v $(pwd):/tmp aquasec/trivy image --exit-code 1 --severity HIGH,CRITICAL $IMAGE_NAME:$CI_COMMIT_SHA

deploy-staging:
  stage: deploy-staging
  image: bitnami/kubectl:latest
  environment:
    name: staging
    url: https://staging.myapp.com
  script:
    - echo $KUBECONFIG_STAGING | base64 -d > $KUBECONFIG
    - kubectl config use-context staging
    - envsubst < k8s/deployment.yaml | kubectl apply -f -
    - kubectl rollout status deployment/myapp -n staging
    - kubectl get services -n staging
  only:
    - develop

deploy-production:
  stage: deploy-production
  image: bitnami/kubectl:latest
  environment:
    name: production
    url: https://myapp.com
  script:
    - echo $KUBECONFIG_PRODUCTION | base64 -d > $KUBECONFIG
    - kubectl config use-context production
    - envsubst < k8s/deployment.yaml | kubectl apply -f -
    - kubectl rollout status deployment/myapp -n production
  when: manual
  only:
    - main
```

### GitHub Actions Workflow

```yaml
# .github/workflows/deploy.yml
name: Deploy to Kubernetes

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  build-and-push:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write

    steps:
    - name: Checkout
      uses: actions/checkout@v3

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v2

    - name: Login to Container Registry
      uses: docker/login-action@v2
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}

    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v4
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=sha,prefix={{branch}}-
          type=raw,value=latest

    - name: Build and push
      uses: docker/build-push-action@v4
      with:
        context: .
        platforms: linux/amd64,linux/arm64
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

  security-scan:
    runs-on: ubuntu-latest
    needs: build-and-push
    steps:
    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        image-ref: '${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }}'
        format: 'sarif'
        output: 'trivy-results.sarif'

    - name: Upload Trivy scan results
      uses: github/codeql-action/upload-sarif@v2
      with:
        sarif_file: 'trivy-results.sarif'

  deploy:
    runs-on: ubuntu-latest
    needs: [build-and-push, security-scan]
    if: github.ref == 'refs/heads/main'
    environment: production

    steps:
    - name: Checkout
      uses: actions/checkout@v3

    - name: Configure kubectl
      uses: azure/setup-kubectl@v3
      with:
        version: 'latest'

    - name: Set up kubeconfig
      run: |
        mkdir -p ~/.kube
        echo "${{ secrets.KUBECONFIG }}" | base64 -d > ~/.kube/config

    - name: Deploy to Kubernetes
      run: |
        sed -i "s/IMAGE_TAG/${{ github.sha }}/g" k8s/deployment.yaml
        kubectl apply -f k8s/
        kubectl rollout status deployment/myapp -n production
        kubectl get services -n production
```

### ArgoCD GitOps

```yaml
# argocd-application.yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: myapp
  namespace: argocd
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  project: default
  source:
    repoURL: https://github.com/myorg/myapp-config
    targetRevision: HEAD
    path: k8s/overlays/production
  destination:
    server: https://kubernetes.default.svc
    namespace: production
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
      allowEmpty: false
    syncOptions:
    - CreateNamespace=true
    - PrunePropagationPolicy=foreground
    - PruneLast=true
  revisionHistoryLimit: 3

---
# Kustomization for environment-specific configs
# k8s/overlays/production/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

namespace: production

resources:
- ../../base

patchesStrategicMerge:
- replica-count.yaml
- resource-limits.yaml

images:
- name: myapp
  newTag: v1.2.3

configMapGenerator:
- name: app-config
  literals:
  - ENV=production
  - DEBUG=false
  - REPLICAS=5

secretGenerator:
- name: app-secrets
  files:
  - database-password.txt
  - api-key.txt
```

---

## 12. Production Best Practices {#best-practices}

### Resource Management

```yaml
# Pod Disruption Budget
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: api-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: api

---
# Horizontal Pod Autoscaler
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: api-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api-deployment
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60

---
# Vertical Pod Autoscaler (VPA)
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: api-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api-deployment
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: api
      minAllowed:
        cpu: 100m
        memory: 128Mi
      maxAllowed:
        cpu: 2
        memory: 2Gi
```

### Multi-Environment Setup

```yaml
# Base deployment
# k8s/base/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: app
        image: myapp:latest
        ports:
        - containerPort: 3000
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi

---
# Production overlay
# k8s/overlays/production/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

namespace: production

resources:
- ../../base

replicas:
- name: myapp
  count: 10

patchesJson6902:
- target:
    group: apps
    version: v1
    kind: Deployment
    name: myapp
  path: resource-limits.yaml

---
# k8s/overlays/production/resource-limits.yaml
- op: replace
  path: /spec/template/spec/containers/0/resources/limits/cpu
  value: "2"
- op: replace
  path: /spec/template/spec/containers/0/resources/limits/memory
  value: "2Gi"
- op: replace
  path: /spec/template/spec/containers/0/resources/requests/cpu
  value: "500m"
- op: replace
  path: /spec/template/spec/containers/0/resources/requests/memory
  value: "1Gi"
```

### Backup and Disaster Recovery

```yaml
# Velero Backup
# Install: velero install --provider aws --plugins velero/velero-plugin-for-aws:v1.7.0 --bucket my-backup-bucket

apiVersion: velero.io/v1
kind: Schedule
metadata:
  name: daily-backup
spec:
  schedule: "0 2 * * *"  # Daily at 2 AM
  template:
    includedNamespaces:
    - production
    - staging
    excludedResources:
    - events
    - events.events.k8s.io
    ttl: "720h"  # 30 days
    storageLocation: default
    volumeSnapshotLocations:
    - default

---
# ETCD Backup Script
apiVersion: batch/v1
kind: CronJob
metadata:
  name: etcd-backup
  namespace: kube-system
spec:
  schedule: "0 */6 * * *"  # Every 6 hours
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: etcd-backup
            image: bitnami/etcd:latest
            command:
            - /bin/sh
            - -c
            - |
              ETCDCTL_API=3 etcdctl snapshot save /backup/etcd-snapshot-$(date +%Y%m%d%H%M%S).db \
                --endpoints=https://127.0.0.1:2379 \
                --cacert=/etc/kubernetes/pki/etcd/ca.crt \
                --cert=/etc/kubernetes/pki/etcd/server.crt \
                --key=/etc/kubernetes/pki/etcd/server.key
              # Upload to cloud storage
              aws s3 cp /backup/ s3://my-etcd-backups/ --recursive
            volumeMounts:
            - name: etcd-certs
              mountPath: /etc/kubernetes/pki/etcd
            - name: backup-storage
              mountPath: /backup
          volumes:
          - name: etcd-certs
            hostPath:
              path: /etc/kubernetes/pki/etcd
          - name: backup-storage
            emptyDir: {}
          restartPolicy: OnFailure
          hostNetwork: true
```

---

## 13. Troubleshooting {#troubleshooting}

### Essential Debug Commands

```bash
# Cluster Information
kubectl cluster-info
kubectl get nodes -o wide
kubectl describe node <node-name>
kubectl top nodes
kubectl get events --sort-by=.metadata.creationTimestamp

# Pod Debugging
kubectl get pods -o wide --all-namespaces
kubectl describe pod <pod-name>
kubectl logs <pod-name> -c <container-name> --previous
kubectl exec -it <pod-name> -- /bin/bash
kubectl port-forward <pod-name> 8080:80

# Service Debugging
kubectl get svc -o wide
kubectl describe svc <service-name>
kubectl get endpoints <service-name>

# Deployment Issues
kubectl get deploy -o wide
kubectl describe deploy <deployment-name>
kubectl rollout status deploy/<deployment-name>
kubectl rollout history deploy/<deployment-name>
kubectl rollout undo deploy/<deployment-name>

# Resource Usage
kubectl top pods --all-namespaces
kubectl top nodes
kubectl describe node <node-name> | grep -A 5 "Allocated resources"

# Network Debugging
kubectl run debug --image=nicolaka/netshoot -it --rm -- /bin/bash
kubectl exec -it debug -- nslookup kubernetes.default
kubectl exec -it debug -- curl -I http://service-name:port

# Storage Issues
kubectl get pv,pvc --all-namespaces
kubectl describe pv <pv-name>
kubectl describe pvc <pvc-name>

# RBAC Issues
kubectl auth can-i <verb> <resource> --as=<user>
kubectl auth can-i create pods --as=system:serviceaccount:default:mysa
kubectl describe clusterrolebinding
```

### Common Issues and Solutions

```yaml
# Issue: ImagePullBackOff
# Solution: Check image name, registry credentials, network connectivity

# Debug ImagePull issues
apiVersion: v1
kind: Pod
metadata:
  name: debug-image-pull
spec:
  containers:
  - name: debug
    image: busybox
    command: ['sh', '-c', 'echo "Checking image pull"; sleep 3600']
  imagePullSecrets:
  - name: registry-secret

---
# Issue: CrashLoopBackOff
# Solution: Check logs, resource limits, health checks

# Debug CrashLoop
kubectl logs <pod-name> --previous
kubectl describe pod <pod-name>
kubectl get events --field-selector involvedObject.name=<pod-name>

---
# Issue: Pending Pods
# Solution: Check node resources, taints, tolerations, affinity rules

# Check scheduling issues
kubectl describe pod <pod-name> | grep -A 10 "Events:"
kubectl get nodes --show-labels
kubectl describe node <node-name> | grep -A 5 "Taints:"

---
# Issue: Service not accessible
# Solution: Check labels, selectors, endpoints, network policies

# Debug service connectivity
kubectl get svc <service-name> -o yaml
kubectl get endpoints <service-name>
kubectl run debug --image=busybox -it --rm -- wget -qO- http://<service-name>:<port>
```

### Performance Debugging

```bash
# Resource constraints
kubectl describe pod <pod-name> | grep -A 10 "Limits:"
kubectl top pod <pod-name> --containers

# Network latency
kubectl run netshoot --image=nicolaka/netshoot -it --rm -- /bin/bash
# Inside the container:
curl -w "@curl-format.txt" -o /dev/null -s "http://service-name:port/"

# DNS resolution
kubectl run debug --image=busybox -it --rm -- nslookup kubernetes.default
kubectl get pods -n kube-system | grep dns

# Storage performance
kubectl run fio --image=dduportal/fio --rm -it -- fio --name=random-write --ioengine=libaio --rw=randwrite --bs=4k --numjobs=1 --size=1g --iodepth=1 --runtime=60 --time_based --end_fsync=1
```

---

## 14. Migration from Docker Swarm {#migration}

### Conceptual Migration Map

```bash
# Docker Swarm → Kubernetes Migration Guide

# 1. Services → Deployments + Services
docker service create --name web --replicas 3 nginx
# Becomes:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
      - name: web
        image: nginx
---
apiVersion: v1
kind: Service
metadata:
  name: web
spec:
  selector:
    app: web
  ports:
  - port: 80

# 2. Stacks → Multiple K8s Resources
docker stack deploy -c docker-compose.yml myapp
# Becomes multiple YAML files or Helm chart

# 3. Networks → Services + Network Policies
docker network create --driver overlay mynet
# Becomes automatic service discovery + optional NetworkPolicies

# 4. Secrets/Configs → Secrets/ConfigMaps (similar)
docker secret create mysecret -
# Becomes:
kubectl create secret generic mysecret --from-literal=key=value
```

### Step-by-Step Migration Process

```yaml
# Phase 1: Preparation
# 1. Inventory your Swarm services
docker service ls > swarm-inventory.txt
docker stack ls >> swarm-inventory.txt

# 2. Document dependencies
# Create dependency map of your services

# 3. Choose migration strategy:
#    - Big Bang (all at once)
#    - Strangler Fig (gradual replacement)
#    - Blue-Green (parallel environments)

# Phase 2: Convert Docker Compose to K8s
# Use kompose tool for initial conversion
curl -L https://github.com/kubernetes/kompose/releases/latest/download/kompose-linux-amd64 -o kompose
chmod +x kompose
sudo mv kompose /usr/local/bin/

# Convert docker-compose.yml to k8s manifests
kompose convert -f docker-compose.yml

# Phase 3: Enhance K8s Manifests
# Add proper resource limits, health checks, etc.

# Example enhancement:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web
  labels:
    app: web
    migrated-from: swarm
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
      - name: web
        image: nginx:1.20
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi
        livenessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 5

# Phase 4: Data Migration
# Plan for stateful services migration
# Use tools like Velero for volume migration
```

### Migration Checklist

```markdown
## Pre-Migration
- [ ] Document all Swarm services and dependencies
- [ ] Set up Kubernetes cluster
- [ ] Install necessary tools (kubectl, helm, etc.)
- [ ] Create namespaces and RBAC
- [ ] Set up monitoring and logging
- [ ] Plan data migration strategy

## During Migration
- [ ] Convert Docker Compose files to K8s manifests
- [ ] Enhance manifests with K8s-specific features
- [ ] Set up ingress controllers
- [ ] Configure persistent storage
- [ ] Migrate secrets and configs
- [ ] Test service-to-service communication
- [ ] Validate health checks and monitoring

## Post-Migration
- [ ] Update DNS records
- [ ] Monitor application performance
- [ ] Set up backup and disaster recovery
- [ ] Update CI/CD pipelines
- [ ] Train team on K8s operations
- [ ] Decommission Swarm cluster
```

---

## 15. Advanced Patterns and Tools {#advanced}

### Operators and Custom Resources

```yaml
# Custom Resource Definition
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: webapps.example.com
spec:
  group: example.com
  versions:
  - name: v1
    served: true
    storage: true
    schema:
      openAPIV3Schema:
        type: object
        properties:
          spec:
            type: object
            properties:
              replicas:
                type: integer
                minimum: 1
                maximum: 100
              image:
                type: string
              port:
                type: integer
          status:
            type: object
  scope: Namespaced
  names:
    plural: webapps
    singular: webapp
    kind: WebApp

---
# Custom Resource Instance
apiVersion: example.com/v1
kind: WebApp
metadata:
  name: my-webapp
spec:
  replicas: 3
  image: nginx:1.20
  port: 80
```

### Helm Charts

```yaml
# Chart.yaml
apiVersion: v2
name: myapp
description: A Helm chart for MyApp
type: application
version: 1.0.0
appVersion: "1.16.0"

dependencies:
- name: postgresql
  version: 11.6.12
  repository: https://charts.bitnami.com/bitnami
  condition: postgresql.enabled
- name: redis
  version: 16.13.2
  repository: https://charts.bitnami.com/bitnami
  condition: redis.enabled

---
# values.yaml
replicaCount: 3

image:
  repository: myapp
  pullPolicy: IfNotPresent
  tag: "latest"

service:
  type: ClusterIP
  port: 80

ingress:
  enabled: true
  className: "nginx"
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
  hosts:
    - host: myapp.example.com
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls:
    - secretName: myapp-tls
      hosts:
        - myapp.example.com

resources:
  limits:
    cpu: 500m
    memory: 512Mi
  requests:
    cpu: 250m
    memory: 256Mi

autoscaling:
  enabled: true
  minReplicas: 3
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80

postgresql:
  enabled: true
  auth:
    postgresPassword: "changeme"
    database: "myapp"

redis:
  enabled: true
  auth:
    enabled: false

---
# templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "myapp.fullname" . }}
  labels:
    {{- include "myapp.labels" . | nindent 4 }}
spec:
  {{- if not .Values.autoscaling.enabled }}
  replicas: {{ .Values.replicaCount }}
  {{- end }}
  selector:
    matchLabels:
      {{- include "myapp.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      labels:
        {{- include "myapp.selectorLabels" . | nindent 8 }}
    spec:
      containers:
        - name: {{ .Chart.Name }}
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: http
          readinessProbe:
            httpGet:
              path: /
              port: http
          resources:
            {{- toYaml .Values.resources | nindent 12 }}
```

### Advanced Scheduling

```yaml
# Node Affinity
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cpu-intensive-app
spec:
  template:
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-type
                operator: In
                values:
                - compute-optimized
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
              - key: zone
                operator: In
                values:
                - us-west-1a
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: database
            topologyKey: kubernetes.io/hostname
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: cpu-intensive-app
              topologyKey: kubernetes.io/hostname

---
# Taints and Tolerations
# On node: kubectl taint nodes node1 dedicated=gpu:NoSchedule

apiVersion: apps/v1
kind: Deployment
metadata:
  name: gpu-workload
spec:
  template:
    spec:
      tolerations:
      - key: "dedicated"
        operator: "Equal"
        value: "gpu"
        effect: "NoSchedule"
      nodeSelector:
        accelerator: nvidia-tesla-k80
```

### GitOps with ArgoCD

```yaml
# ArgoCD Application of Applications pattern
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: app-of-apps
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/myorg/k8s-apps
    targetRevision: HEAD
    path: applications
  destination:
    server: https://kubernetes.default.svc
    namespace: argocd
  syncPolicy:
    automated:
      prune: true
      selfHeal: true

---
# Individual application
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: myapp-prod
  namespace: argocd
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  project: default
  source:
    repoURL: https://github.com/myorg/myapp-config
    targetRevision: HEAD
    path: environments/production
    helm:
      valueFiles:
      - values-prod.yaml
      parameters:
      - name: image.tag
        value: v1.2.3
  destination:
    server: https://kubernetes.default.svc
    namespace: production
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
    - CreateNamespace=true
    - PruneLast=true
  revisionHistoryLimit: 10
```

### Service Mesh (Istio)

```yaml
# Install Istio
# curl -L https://istio.io/downloadIstio | sh -
# istioctl install --set values.defaultRevision=default

# Gateway
apiVersion: networking.istio.io/v1beta1
kind: Gateway
metadata:
  name: myapp-gateway
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 443
      name: https
      protocol: HTTPS
    tls:
      mode: SIMPLE
      credentialName: myapp-cert
    hosts:
    - myapp.example.com

---
# Virtual Service with Canary Deployment
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: myapp-vs
spec:
  hosts:
  - myapp.example.com
  gateways:
  - myapp-gateway
  http:
  - match:
    - headers:
        canary:
          exact: "true"
    route:
    - destination:
        host: myapp-service
        subset: v2
  - route:
    - destination:
        host: myapp-service
        subset: v1
      weight: 90
    - destination:
        host: myapp-service
        subset: v2
      weight: 10

---
# Destination Rule with Circuit Breaker
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: myapp-dr
spec:
  host: myapp-service
  trafficPolicy:
    connectionPool:
      tcp:
        maxConnections: 100
      http:
        http1MaxPendingRequests: 10
        maxRequestsPerConnection: 2
    circuitBreaker:
      consecutiveGatewayErrors: 5
      interval: 10s
      baseEjectionTime: 30s
      maxEjectionPercent: 50
    loadBalancer:
      simple: LEAST_CONN
  subsets:
  - name: v1
    labels:
      version: v1
  - name: v2
    labels:
      version: v2

---
# Security Policy
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: default
spec:
  mtls:
    mode: STRICT

---
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: myapp-authz
spec:
  selector:
    matchLabels:
      app: myapp
  rules:
  - from:
    - source:
        principals: ["cluster.local/ns/frontend/sa/frontend-sa"]
  - to:
    - operation:
        methods: ["GET", "POST"]
```

---

## Conclusion

Kubernetes provides a powerful platform for container orchestration with extensive capabilities beyond Docker Swarm. This guide covers the essential knowledge needed to successfully migrate from Docker Swarm to Kubernetes and operate production workloads.

### Key Differences from Docker Swarm:
- **Complexity**: More complex but more powerful
- **Declarative**: Everything is defined as desired state
- **Ecosystem**: Vast third-party tools and operators
- **Scalability**: Better for large, complex applications
- **Flexibility**: Multiple deployment patterns and strategies

### Next Steps for Production:
1. **Start Small**: Begin with simple applications
2. **Learn Gradually**: Master core concepts before advanced features
3. **Use Managed Services**: Consider EKS, GKE, or AKS for production
4. **Implement GitOps**: Use ArgoCD or Flux for deployment automation
5. **Monitor Everything**: Set up comprehensive observability
6. **Security First**: Implement RBAC, network policies, and security scanning
7. **Plan for Scale**: Use HPA, VPA, and cluster autoscaling

### Essential Tools to Master:
- **kubectl**: Command-line tool
- **Helm**: Package manager
- **Kustomize**: Configuration management
- **ArgoCD/Flux**: GitOps
- **Prometheus/Grafana**: Monitoring
- **Istio**: Service mesh (for advanced use cases)

Remember: Kubernetes has a steeper learning curve than Docker Swarm, but the investment pays off with greater flexibility, scalability, and ecosystem support for complex production workloads.
